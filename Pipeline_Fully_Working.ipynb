{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install hydra-core==0.11.3\n",
        "%pip install omegaconf==1.4.1\n",
        "%pip install loguru==0.5.0"
      ],
      "metadata": {
        "id": "e9-KKdaxCHYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31eb63fb-8894-4ea7-a651-9d450dc03dc5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hydra-core==0.11.3\n",
            "  Downloading hydra_core-0.11.3-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting omegaconf<1.5,>=1.4 (from hydra-core==0.11.3)\n",
            "  Downloading omegaconf-1.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from omegaconf<1.5,>=1.4->hydra-core==0.11.3) (1.17.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from omegaconf<1.5,>=1.4->hydra-core==0.11.3) (6.0.2)\n",
            "Downloading hydra_core-0.11.3-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-1.4.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: omegaconf, hydra-core\n",
            "Successfully installed hydra-core-0.11.3 omegaconf-1.4.1\n",
            "Requirement already satisfied: omegaconf==1.4.1 in /usr/local/lib/python3.11/dist-packages (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from omegaconf==1.4.1) (1.17.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from omegaconf==1.4.1) (6.0.2)\n",
            "Collecting loguru==0.5.0\n",
            "  Downloading loguru-0.5.0-py3-none-any.whl.metadata (20 kB)\n",
            "Downloading loguru-0.5.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru\n",
            "Successfully installed loguru-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5nBoNooNB6kU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import multiprocessing as mp\n",
        "import itertools\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/riktor/KGPL/"
      ],
      "metadata": {
        "id": "-3q187KhB8up",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ddf10da-bd39-453e-ee60-b1872b4be88e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KGPL'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 59 (delta 14), reused 55 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (59/59), 13.90 MiB | 7.85 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n",
            "Updating files: 100% (26/26), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd KGPL &&\\\n",
        "python preprocess/preprocess.py -d \"music\" &&\\\n",
        "python preprocess/make_path_list.py lp_depth=6 dataset=music kg_path=data/music/kg_final.npy rating_path=data/music/ratings_final.npy num_neighbor_samples=32"
      ],
      "metadata": {
        "id": "6TJnHlXfB-Iu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fc8ec91-dc2e-46da-b749-41f3a278362e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading item index to entity id file: data/music/item_index2entity_id.txt ...\n",
            "reading rating file ...\n",
            "converting rating file ...\n",
            "number of users: 1872\n",
            "number of items: 3846\n",
            "converting kg file ...\n",
            "number of entities (containing items): 9366\n",
            "number of relations: 60\n",
            "done\n",
            "adj_entity_path: data/music/adj_entity_6_32.npy\n",
            "adj_relation_path: data/music/adj_relation_6_32.npy\n",
            "data_path: data/music/fold1.pkl\n",
            "dataset: music\n",
            "kg_path: data/music/kg_final.npy\n",
            "lp_depth: 6\n",
            "num_neighbor_samples: 32\n",
            "pathlist_path: data/music/path_list_6_32.pkl\n",
            "rating_path: data/music/ratings_final.npy\n",
            "reachable_items_path: data/music/reachable_items.pkl\n",
            "\n",
            "[Parallel(n_jobs=32)]: Using backend MultiprocessingBackend with 32 concurrent workers.\n",
            "[Parallel(n_jobs=32)]: Batch computation too fast (0.055136680603027344s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=32)]: Done   8 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=32)]: Done  21 tasks      | elapsed:    2.6s\n",
            "[Parallel(n_jobs=32)]: Done  34 tasks      | elapsed:    5.2s\n",
            "[Parallel(n_jobs=32)]: Batch computation too slow (8.651839017868042s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=32)]: Done  51 tasks      | elapsed:   10.2s\n",
            "[Parallel(n_jobs=32)]: Done  70 tasks      | elapsed:   14.6s\n",
            "[Parallel(n_jobs=32)]: Done 102 tasks      | elapsed:   20.8s\n",
            "[Parallel(n_jobs=32)]: Done 133 tasks      | elapsed:   28.9s\n",
            "[Parallel(n_jobs=32)]: Done 162 tasks      | elapsed:   31.5s\n",
            "[Parallel(n_jobs=32)]: Done 186 tasks      | elapsed:   34.9s\n",
            "[Parallel(n_jobs=32)]: Done 210 tasks      | elapsed:   41.6s\n",
            "[Parallel(n_jobs=32)]: Done 234 tasks      | elapsed:   46.5s\n",
            "[Parallel(n_jobs=32)]: Done 258 tasks      | elapsed:   49.8s\n",
            "[Parallel(n_jobs=32)]: Done 283 tasks      | elapsed:   56.3s\n",
            "[Parallel(n_jobs=32)]: Done 310 tasks      | elapsed:  1.0min\n",
            "[Parallel(n_jobs=32)]: Done 335 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=32)]: Done 362 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=32)]: Done 391 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=32)]: Done 420 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=32)]: Done 449 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=32)]: Done 480 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=32)]: Done 511 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=32)]: Done 544 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=32)]: Done 577 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=32)]: Done 612 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=32)]: Done 647 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=32)]: Done 685 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=32)]: Done 722 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=32)]: Done 761 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=32)]: Done 800 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=32)]: Done 841 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs=32)]: Done 882 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=32)]: Done 925 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=32)]: Done 968 tasks      | elapsed:  3.3min\n",
            "[Parallel(n_jobs=32)]: Done 1013 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=32)]: Done 1058 tasks      | elapsed:  3.5min\n",
            "[Parallel(n_jobs=32)]: Done 1105 tasks      | elapsed:  3.7min\n",
            "[Parallel(n_jobs=32)]: Done 1152 tasks      | elapsed:  3.8min\n",
            "[Parallel(n_jobs=32)]: Done 1201 tasks      | elapsed:  4.0min\n",
            "[Parallel(n_jobs=32)]: Done 1250 tasks      | elapsed:  4.2min\n",
            "[Parallel(n_jobs=32)]: Done 1301 tasks      | elapsed:  4.3min\n",
            "[Parallel(n_jobs=32)]: Done 1352 tasks      | elapsed:  4.6min\n",
            "[Parallel(n_jobs=32)]: Done 1405 tasks      | elapsed:  4.7min\n",
            "[Parallel(n_jobs=32)]: Done 1458 tasks      | elapsed:  4.9min\n",
            "[Parallel(n_jobs=32)]: Done 1513 tasks      | elapsed:  5.1min\n",
            "[Parallel(n_jobs=32)]: Done 1568 tasks      | elapsed:  5.2min\n",
            "[Parallel(n_jobs=32)]: Done 1625 tasks      | elapsed:  5.5min\n",
            "[Parallel(n_jobs=32)]: Done 1682 tasks      | elapsed:  5.7min\n",
            "[Parallel(n_jobs=32)]: Done 1741 tasks      | elapsed:  5.9min\n",
            "[Parallel(n_jobs=32)]: Done 1800 tasks      | elapsed:  6.1min\n",
            "[Parallel(n_jobs=32)]: Done 1861 tasks      | elapsed:  6.3min\n",
            "[Parallel(n_jobs=32)]: Done 1922 tasks      | elapsed:  6.5min\n",
            "[Parallel(n_jobs=32)]: Done 1985 tasks      | elapsed:  6.7min\n",
            "[Parallel(n_jobs=32)]: Done 2048 tasks      | elapsed:  6.8min\n",
            "[Parallel(n_jobs=32)]: Done 2113 tasks      | elapsed:  7.1min\n",
            "[Parallel(n_jobs=32)]: Done 2178 tasks      | elapsed:  7.2min\n",
            "[Parallel(n_jobs=32)]: Done 2245 tasks      | elapsed:  7.5min\n",
            "[Parallel(n_jobs=32)]: Done 2312 tasks      | elapsed:  7.7min\n",
            "[Parallel(n_jobs=32)]: Done 2381 tasks      | elapsed:  8.0min\n",
            "[Parallel(n_jobs=32)]: Done 2450 tasks      | elapsed:  8.3min\n",
            "[Parallel(n_jobs=32)]: Done 2521 tasks      | elapsed:  8.5min\n",
            "[Parallel(n_jobs=32)]: Done 2592 tasks      | elapsed:  8.8min\n",
            "[Parallel(n_jobs=32)]: Done 2665 tasks      | elapsed:  9.1min\n",
            "[Parallel(n_jobs=32)]: Done 2738 tasks      | elapsed:  9.5min\n",
            "[Parallel(n_jobs=32)]: Done 2813 tasks      | elapsed:  9.8min\n",
            "[Parallel(n_jobs=32)]: Done 2888 tasks      | elapsed:  9.9min\n",
            "[Parallel(n_jobs=32)]: Done 2965 tasks      | elapsed: 10.2min\n",
            "[Parallel(n_jobs=32)]: Done 3042 tasks      | elapsed: 10.5min\n",
            "[Parallel(n_jobs=32)]: Done 3121 tasks      | elapsed: 10.9min\n",
            "[Parallel(n_jobs=32)]: Done 3200 tasks      | elapsed: 11.1min\n",
            "[Parallel(n_jobs=32)]: Done 3281 tasks      | elapsed: 11.4min\n",
            "[Parallel(n_jobs=32)]: Done 3362 tasks      | elapsed: 11.7min\n",
            "[Parallel(n_jobs=32)]: Done 3445 tasks      | elapsed: 11.9min\n",
            "[Parallel(n_jobs=32)]: Done 3528 tasks      | elapsed: 12.3min\n",
            "[Parallel(n_jobs=32)]: Done 3613 tasks      | elapsed: 12.6min\n",
            "[Parallel(n_jobs=32)]: Done 3698 tasks      | elapsed: 12.8min\n",
            "[Parallel(n_jobs=32)]: Done 3846 out of 3846 | elapsed: 13.4min finished\n",
            "average number of paths: 552.0413416536661\n",
            "median number of paths: 255.0\n",
            "min number of paths: 2\n",
            "max number of paths: 8522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "bmQ4fOo3bQrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_dst_dict(path_list_dict):\n",
        "        \"\"\"\n",
        "        Transform path representations:\n",
        "        `list of nodes` to `dictionary of source to sink (dst_dict)`\n",
        "        \"\"\"\n",
        "        print(\"Setting up dst dict...\")\n",
        "        dst_dict = {}\n",
        "        for item in tqdm(path_list_dict):\n",
        "            dst = []\n",
        "            paths = path_list_dict[item]\n",
        "            for p in paths:\n",
        "                dst.append(p[-1])\n",
        "            dst_dict[item] = Counter(dst)\n",
        "\n",
        "        print(\"Start updating path info...\")\n",
        "        print(\"Path info updated.\")\n",
        "        return dst_dict\n",
        "\n",
        "def grouper(n, iterable, squash=None):\n",
        "    it = iter(iterable)\n",
        "    while True:\n",
        "        if squash:\n",
        "            chunk = [\n",
        "                [None if (j != 0 and i in squash) else el[i] for i in range(len(el))]\n",
        "                for j, el in enumerate(itertools.islice(it, n))\n",
        "            ]\n",
        "        else:\n",
        "            chunk = list(itertools.islice(it, n))\n",
        "\n",
        "        if not chunk:\n",
        "            return\n",
        "        elif len(chunk) != n:\n",
        "            chunk += [None] * (n - len(chunk))\n",
        "        yield chunk\n",
        "\n",
        "def compute_reachable_items_(args_list):\n",
        "  \"\"\"Construct the sampling distributions based on paths in KG.\n",
        "  Args:\n",
        "      args_list: list of list of arguments. Each arguments' list must contains;\n",
        "      (1) user_id;\n",
        "      (2) user's interacted item ids (seed items);\n",
        "      (3) item-to-(item, #paths) dict found in the BFS (start and end points of some paths);\n",
        "      (4) item-to-frequency dict;\n",
        "      (5) power coefficient to control the skewness of sampling distributions\n",
        "  Returns:\n",
        "      dict in which (key, value) = (item list, np.array of sampling distribution).\n",
        "      sampling distribution is transformed to CDF for fast sampling.\n",
        "  \"\"\"\n",
        "  idd = {}\n",
        "  _, _, dst_dict, item_freq, pn = args_list[0]\n",
        "  for args in args_list:\n",
        "      if args is None:\n",
        "          continue\n",
        "      user, seed_items, _, _, _ = args\n",
        "\n",
        "      # print('User:', user)\n",
        "      # print('Seed Items:', seed_items)\n",
        "\n",
        "      # Collect user's reachable items with the number of reachable paths\n",
        "      dst = Counter()\n",
        "      for item in seed_items:\n",
        "          if item in dst_dict:\n",
        "              dst += dst_dict[item]\n",
        "\n",
        "      if len(dst) != 0:\n",
        "          # Unique reachable items for the user\n",
        "          udst = np.array(tuple(dst.keys()))\n",
        "\n",
        "          # Histogram of paths with power transform\n",
        "          F = np.array(tuple(dst.values())) ** pn\n",
        "\n",
        "          # Remove the seed (positve) items\n",
        "          inds = ~np.isin(udst, seed_items)\n",
        "          udst = udst[inds]\n",
        "          F = F[inds]\n",
        "\n",
        "          # Compute unreachable items and concat those to the end of item lists\n",
        "          udst = set(udst)\n",
        "          unreachable_items = [i for i in item_freq if i not in udst]\n",
        "          udst = list(udst) + unreachable_items\n",
        "\n",
        "          # For unreachable items, assume 0.5 virtual paths for full support\n",
        "          F = np.concatenate([F, np.ones(len(unreachable_items)) * 0.5])\n",
        "\n",
        "          # Transform histogram to CDF\n",
        "          sort_inds = np.argsort(F)\n",
        "          udst = [udst[i] for i in sort_inds]\n",
        "          F = F[sort_inds]\n",
        "          F = (F / np.sum(F)).cumsum()\n",
        "          idd[user] = (udst, F)\n",
        "  return idd\n",
        "\n",
        "# HELPER FUNCTION STRAIGHT FROM CHATGPT\n",
        "def build_user_train_dict_from_tensor(ratings):\n",
        "    user_train = defaultdict(list)\n",
        "    for user, item, rating in ratings:\n",
        "        if rating >= 1:  # or whatever you define as \"positive\" interaction\n",
        "            user = user.item()\n",
        "            item = item.item()\n",
        "            # if user not in user_train:\n",
        "            #     user_train[user] = []\n",
        "            user_train[user].append(item)\n",
        "    return user_train"
      ],
      "metadata": {
        "id": "Wp6sO1ulCAyt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Dataset Class"
      ],
      "metadata": {
        "id": "QbSzL0gWdyhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KGPL_Dataset(Dataset):\n",
        "  '''\n",
        "  Custom dataset class which includes all datasets and parameters per model.\n",
        "  Specified under \"data\" directory\n",
        "  '''\n",
        "\n",
        "  base_data_path = 'KGPL/data/'\n",
        "  datasets = {}\n",
        "  n_user = 0\n",
        "  n_item = 0\n",
        "\n",
        "  class cfg:\n",
        "    plabel_lp_depth = 6\n",
        "    plabel_par = 16\n",
        "    plabel_chunk_size = 250\n",
        "    plabel_neg_pn = 0.1\n",
        "    plabel_pl_pn = 1e-3\n",
        "\n",
        "  @classmethod\n",
        "  def _init_class(cls, dataset_name):\n",
        "    cls.dataset_name = dataset_name\n",
        "    print('Loading Entity Adjacencies...')\n",
        "    cls.adj_entity = torch.from_numpy(np.load(cls.base_data_path + dataset_name + '/adj_entity_6_32.npy'))\n",
        "    print('Loading Relation Adjacencies...')\n",
        "    cls.adj_relation = torch.from_numpy(np.load(cls.base_data_path + dataset_name + '/adj_relation_6_32.npy'))\n",
        "    print('Loading Ratings...')\n",
        "    cls.ratings = torch.from_numpy(np.load(cls.base_data_path + dataset_name + '/ratings_final.npy'))\n",
        "    print('Loading Path List...')\n",
        "    cls.path_list_dict = pickle.load(open(cls.base_data_path + dataset_name + '/path_list_6_32.pkl', 'rb'))\n",
        "    print('Loading Distances...')\n",
        "    cls.dst_dict = setup_dst_dict(cls.path_list_dict)\n",
        "\n",
        "  def _split_data(self, split_ratio=0.2):\n",
        "    #split dataset\n",
        "    n_ratings = len(self.ratings)\n",
        "    split_indices = torch.randperm(n_ratings)[:int(n_ratings * split_ratio)]\n",
        "    splitted_data = self.ratings[split_indices]\n",
        "    rest_data = self.ratings[~torch.isin(torch.arange(n_ratings), split_indices)]\n",
        "    #create new objects\n",
        "    splitted_dataset, rest_dataset = deepcopy(self), deepcopy(self)\n",
        "    splitted_dataset.ratings = splitted_data\n",
        "    rest_dataset.ratings = rest_data\n",
        "    return rest_dataset, splitted_dataset\n",
        "\n",
        "  def _readjust_counts(self):\n",
        "    self.users = torch.unique(self.ratings[:,0])\n",
        "    self.items = torch.unique(self.ratings[:,1])\n",
        "\n",
        "  def _fix_set_for_positives(self):\n",
        "    '''\n",
        "    Train set needs at least one positive example per user.\n",
        "    '''\n",
        "    # Step 1: Get unique groups\n",
        "    groups = self.ratings[:, 0].unique()\n",
        "\n",
        "    # Step 2: Find valid groups\n",
        "    valid_groups = []\n",
        "    for g in groups:\n",
        "        # mask for current group\n",
        "        mask = self.ratings[:, 0] == g\n",
        "        # check if any third column > 0\n",
        "        if (self.ratings[mask][:, 2] > 0).any():\n",
        "            valid_groups.append(g)\n",
        "    valid_groups = torch.Tensor(valid_groups)\n",
        "    self.ratings = self.ratings[(self.ratings[:, 0][:, None] == valid_groups).any(dim=1)]\n",
        "\n",
        "  def _train_val_test_split(self):\n",
        "    exp_dataset, test = self._split_data()\n",
        "    train, val = exp_dataset._split_data()\n",
        "    train.train_set = True\n",
        "    val.train_set = False\n",
        "    test.train_set = False\n",
        "    for fold in (train, val, test):\n",
        "      fold._readjust_counts()\n",
        "    n_user = torch.unique(self.ratings[:,0]).numel()\n",
        "    n_item = torch.unique(self.ratings[:,1]).numel()\n",
        "    train._fix_set_for_positives()\n",
        "    train._readjust_counts()\n",
        "    val._readjust_counts()\n",
        "    test._readjust_counts()\n",
        "    self.__class__.n_user = n_user\n",
        "    self.__class__.n_item = n_item\n",
        "    self.__class__.datasets['train'] = train\n",
        "    self.__class__.datasets['val'] = val\n",
        "    self.__class__.datasets['test'] = test\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def _build_freq_dict(seq, all_candidates): # need all_candidates\n",
        "    _freq = Counter(seq)\n",
        "    for i in all_candidates:\n",
        "        if i not in _freq:\n",
        "            _freq[i] += 1\n",
        "    freq = [_freq[i] for i in all_candidates]\n",
        "    return dict(zip(all_candidates, freq))\n",
        "\n",
        "  #### THIS IS THE BIG ONE #####\n",
        "  def set_item_candidates(self):\n",
        "        \"\"\"\n",
        "        Construct the sampling distrbiutions for negative/pseudo-labelled instances for each user\n",
        "        \"\"\"\n",
        "        train_data = self.datasets['train'].ratings\n",
        "        eval_data = self.datasets['val'].ratings\n",
        "        # all_users = tuple(set(train_data[:, 0])) # train data sets the users - not predicting unseen users\n",
        "        all_users = torch.unique(train_data[:,0])\n",
        "        print('All Users (In Training Set):', len(all_users))\n",
        "        self.train_users = all_users\n",
        "        self.all_items = set(torch.arange(self.__class__.n_item))\n",
        "        self.neg_c_dict_user = self._build_freq_dict(\n",
        "            torch.concat([train_data[:, 0], eval_data[:, 0]]), self.train_users\n",
        "        )\n",
        "\n",
        "        print('Neg C Dict User:', len(self.neg_c_dict_user))\n",
        "\n",
        "        self.neg_c_dict_item = self._build_freq_dict(\n",
        "            np.concatenate([train_data[:, 1], eval_data[:, 1]]), self.all_items\n",
        "        )\n",
        "\n",
        "        print('Neg C Dict Item:', len(self.neg_c_dict_item))\n",
        "\n",
        "        item_cands = tuple(self.neg_c_dict_item.keys())\n",
        "        F = np.array(tuple(self.neg_c_dict_item.values())) ** self.cfg.plabel_neg_pn\n",
        "        sort_inds = np.argsort(F)\n",
        "        item_cands = [item_cands[i] for i in sort_inds]\n",
        "        F = F[sort_inds]\n",
        "        F = (F / F.sum()).cumsum()\n",
        "        self.item_freq = (item_cands, F)\n",
        "\n",
        "        for u, i in tqdm(train_data[:, 0:2]):\n",
        "            self.user_seed_dict[u.item()].add(i.item())\n",
        "\n",
        "        # path = hydra.utils.to_absolute_path(self.cfg.reachable_items_path)\n",
        "        # logger.info(\"calculating reachable items for users\")\n",
        "        # self._setup_dst_dict(self.path_list_dict)\n",
        "        item_dist_dict = {}\n",
        "        src_itr = map(\n",
        "            lambda iu: (\n",
        "                all_users[iu].item(),\n",
        "                tuple(self.user_seed_dict[all_users[iu].item()]),\n",
        "                self.dst_dict,\n",
        "                self.neg_c_dict_item,\n",
        "                self.cfg.plabel_pl_pn,\n",
        "            ),\n",
        "            range(len(all_users)),\n",
        "        )\n",
        "\n",
        "        grouped = grouper(self.cfg.plabel_chunk_size, src_itr, squash=set([2, 3]))\n",
        "\n",
        "        # --------- commented out multiprocessing --------\n",
        "        # with mp.Pool(self.cfg.plabel_par) as pool:\n",
        "        #     for idd in pool.imap_unordered(compute_reachable_items_, grouped):\n",
        "        #         item_dist_dict.update(idd)\n",
        "        #         print(idd)\n",
        "        print('Populating item dist dict...')\n",
        "        item_dist_dict = {}\n",
        "        for group in tqdm(grouped):\n",
        "            # print('Group sample:', group[0])\n",
        "            idd = compute_reachable_items_(group)\n",
        "            item_dist_dict.update(idd)\n",
        "        self.item_dist_dict = item_dist_dict\n",
        "\n",
        "  def __init__(self, dataset_name):\n",
        "    self._init_class(dataset_name)\n",
        "    self.user_seed_dict = defaultdict(set)\n",
        "    self._train_val_test_split()\n",
        "    self.__class__.datasets['train'].set_item_candidates()\n",
        "    # self.__class__.datasets['train'].user_train = build_user_train_dict_from_tensor(self.__class__.datasets['train'].ratings)\n",
        "\n",
        "  def sample_positive(self, user):\n",
        "      choice_seq = self.ratings[(self.ratings[:,0] == user) & (self.ratings[:,2]) >= 1][:,1]\n",
        "      if choice_seq.numel() == 0:\n",
        "        print('Something went wrong.')\n",
        "      return random.choice(choice_seq)\n",
        "\n",
        "  def sample_negative(self, user):\n",
        "      # seen = set(self.user_train[user])\n",
        "      seen = torch.unique(self.ratings[(self.ratings[:,0] == user)][:,1])\n",
        "      while True:\n",
        "          item = random.randint(0, self.n_item - 1)\n",
        "          if item not in seen:\n",
        "              return torch.tensor(item)\n",
        "\n",
        "  def sample_pseudo_label(self, user):\n",
        "        udst, F = self.item_dist_dict[user.item()]\n",
        "        r = random.random()\n",
        "        F = torch.as_tensor(F)\n",
        "        idx = torch.searchsorted(F, r, right=True)\n",
        "        pl = udst[idx]\n",
        "        if type(pl) == np.int64:\n",
        "            pl = torch.tensor(pl)\n",
        "        return pl\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    if self.train_set:\n",
        "      user = self.train_users[idx]\n",
        "      pos_item = self.sample_positive(user)\n",
        "      neg_item = self.sample_negative(user)\n",
        "      pseudo_label = self.sample_pseudo_label(user)\n",
        "      return user, pos_item, neg_item, pseudo_label\n",
        "    else:\n",
        "      # still need to implement\n",
        "      return None\n",
        "\n",
        "  def __len__(self):\n",
        "    if self.train_set:\n",
        "      return len(self.train_users)\n",
        "    else:\n",
        "      return self.n_user\n",
        "\n",
        "KGPL_Dataset('music')"
      ],
      "metadata": {
        "id": "j65m9YApYsr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d762d136-99f0-4775-bf09-6c1d208f2078"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Entity Adjacencies...\n",
            "Loading Relation Adjacencies...\n",
            "Loading Ratings...\n",
            "Loading Path List...\n",
            "Loading Distances...\n",
            "Setting up dst dict...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3846/3846 [00:00<00:00, 8296.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start updating path info...\n",
            "Path info updated.\n",
            "All Users (In Training Set): 1867\n",
            "Neg C Dict User: 1867\n",
            "Neg C Dict Item: 3846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27089/27089 [00:00<00:00, 47269.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating item dist dict...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8it [00:05,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.KGPL_Dataset at 0x7c600b022650>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(KGPL_Dataset.datasets['train'].train_users)):\n",
        "  try:\n",
        "    _ = KGPL_Dataset.datasets['train'][i]\n",
        "  except:\n",
        "    print(i)\n",
        "    break"
      ],
      "metadata": {
        "id": "GqaqmigmjHxm"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "vVf41zoV2nXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KGPL_Config():\n",
        "  '''\n",
        "  KGPL model configuration for each dataset.\n",
        "  '''\n",
        "  def __init__(self, dataset_name:str, model_type:str, neighbor_sample_size:int, dropout_rate:float, emb_dim:int, n_iter:int, plabel:dict, optimize:dict, log:dict, evaluate:dict, model:dict):\n",
        "    self.dataset_name = dataset_name\n",
        "    self.model_type = model_type\n",
        "    self.neighbor_sample_size = neighbor_sample_size\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.emb_dim = emb_dim\n",
        "    self.n_iter = n_iter\n",
        "    self.plabel = plabel\n",
        "    self.optimize = optimize\n",
        "    self.log = log\n",
        "    self.evaluate = evaluate\n",
        "    self.model = model"
      ],
      "metadata": {
        "id": "Rn6q_sSB2mtS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cfg = KGPL_Config(\n",
        "    'music',\n",
        "    'KGPL_COT',\n",
        "    neighbor_sample_size=32,\n",
        "    dropout_rate=0.5,\n",
        "    emb_dim=64,\n",
        "    n_iter=1,\n",
        "    plabel={},\n",
        "    optimize={'iter_per_epoch':100, 'lr': 3e-3, 'batch_size':3333},\n",
        "    log={'show_loss':True},\n",
        "    evaluate={'user_num_topk':1000},\n",
        "    model={'n_iter':1, 'neighbor_sample_size':32, 'dropout_rate':0.5}\n",
        ")"
      ],
      "metadata": {
        "id": "TrkxkKtMZMDl"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kgpl_loss(pos_scores, neg_scores, pseudo_scores):\n",
        "    # BCE loss like TensorFlow version\n",
        "    pos_labels = torch.ones_like(pos_scores)\n",
        "    neg_labels = torch.zeros_like(neg_scores)\n",
        "    pseudo_labels = torch.ones_like(pseudo_scores)\n",
        "    loss = F.binary_cross_entropy_with_logits(pos_scores, pos_labels) + \\\n",
        "           F.binary_cross_entropy_with_logits(neg_scores, neg_labels) + \\\n",
        "           F.binary_cross_entropy_with_logits(pseudo_scores, pseudo_labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "erpb_BB2QFMT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SumAggregatorWithDropout(nn.Module):\n",
        "    def __init__(self, emb_dim, dropout_rate, activation, cfg):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(emb_dim * 2, emb_dim)\n",
        "        self.activation = activation\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, self_vectors, neighbor_vectors, neighbor_relations, user_embeddings):\n",
        "        # self_vectors: [batch_size, emb_dim]\n",
        "        # neighbor_vectors: [batch_size, n_neighbors, emb_dim]\n",
        "        # neighbor_mean = neighbor_vectors.mean(dim=1)  # [batch_size, emb_dim]\n",
        "        neighbor_mean = neighbor_vectors.mean(dim=[1, 2])\n",
        "        out = torch.cat([self_vectors, neighbor_mean], dim=-1)  # [batch_size, emb_dim * 2]\n",
        "        out = self.linear(out)\n",
        "        out = self.dropout(out)\n",
        "        return self.activation(out)\n",
        "\n",
        "\n",
        "class KGPLStudent(nn.Module):\n",
        "    def __init__(self, cfg, n_user, n_entity, n_relation, adj_entity, adj_relation, path_list_dict, name, eval_mode=False):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.name = name\n",
        "        self.n_user = n_user\n",
        "        self.n_entity = n_entity\n",
        "        self.n_relation = n_relation\n",
        "        self.batch_size = cfg.optimize['batch_size']\n",
        "        self.adj_entity = adj_entity\n",
        "        self.adj_relation = adj_relation\n",
        "        self.path_list_dict = path_list_dict\n",
        "        self.eval_mode = eval_mode\n",
        "\n",
        "        self.user_emb_matrix = nn.Embedding(n_user, cfg.emb_dim)\n",
        "        self.entity_emb_matrix = nn.Embedding(n_entity, cfg.emb_dim)\n",
        "        self.relation_emb_matrix = nn.Embedding(n_relation, cfg.emb_dim)\n",
        "\n",
        "        self.aggregators = nn.ModuleList([\n",
        "            SumAggregatorWithDropout(cfg.emb_dim, cfg.dropout_rate, activation=nn.Tanh(), cfg=cfg)\n",
        "            if i == cfg.n_iter - 1 else\n",
        "            SumAggregatorWithDropout(cfg.emb_dim, cfg.dropout_rate, activation=nn.LeakyReLU(), cfg=cfg)\n",
        "            for i in range(cfg.n_iter)\n",
        "        ])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.user_emb_matrix.weight)\n",
        "        nn.init.xavier_uniform_(self.entity_emb_matrix.weight)\n",
        "        nn.init.xavier_uniform_(self.relation_emb_matrix.weight)\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embeds = self.user_emb_matrix(user_indices)\n",
        "        item_embeds = self.get_item_embeddings(item_indices)\n",
        "        scores = (user_embeds * item_embeds).sum(dim=-1)\n",
        "        return scores\n",
        "\n",
        "    def get_item_embeddings(self, item_indices):\n",
        "        entities = [item_indices]\n",
        "        relations = []\n",
        "        for i in range(self.cfg.n_iter):\n",
        "            neighbor_entities = self.adj_entity[entities[-1]].view(item_indices.size(0), -1)\n",
        "            neighbor_relations = self.adj_relation[entities[-1]].view(item_indices.size(0), -1)\n",
        "            entities.append(neighbor_entities)\n",
        "            relations.append(neighbor_relations)\n",
        "\n",
        "        entity_vectors = [self.entity_emb_matrix(e) for e in entities]\n",
        "        relation_vectors = [self.relation_emb_matrix(r) for r in relations]\n",
        "\n",
        "        for i in range(self.cfg.n_iter):\n",
        "            new_vectors = []\n",
        "            for hop in range(self.cfg.n_iter - i):\n",
        "                batch_size, neighbor_size, emb_dim = entity_vectors[hop+1].size(0), entity_vectors[hop+1].size(1) // self.cfg.model['neighbor_sample_size'], entity_vectors[hop+1].size(2)\n",
        "                neighbor_vecs = entity_vectors[hop+1].view(batch_size, neighbor_size, self.cfg.model['neighbor_sample_size'], emb_dim)\n",
        "                relation_vecs = relation_vectors[hop].view(batch_size, neighbor_size, self.cfg.model['neighbor_sample_size'], emb_dim)\n",
        "                vector = self.aggregators[i](\n",
        "                    self_vectors=entity_vectors[hop],\n",
        "                    neighbor_vectors=neighbor_vecs,\n",
        "                    neighbor_relations=relation_vecs,\n",
        "                    user_embeddings=None  # optional\n",
        "                )\n",
        "                new_vectors.append(vector)\n",
        "            entity_vectors = new_vectors\n",
        "\n",
        "        return entity_vectors[0]"
      ],
      "metadata": {
        "id": "EeA3WTSJ3EPu"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        user, pos_item, neg_item, pseudo_item = batch\n",
        "        user = user.to(device)\n",
        "        pos_item = pos_item.to(device)\n",
        "        neg_item = neg_item.to(device)\n",
        "        pseudo_item = pseudo_item.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pos_scores = model(user, pos_item)\n",
        "        neg_scores = model(user, neg_item)\n",
        "        pseudo_scores = model(user, pseudo_item)\n",
        "\n",
        "        loss = kgpl_loss(pos_scores, neg_scores, pseudo_scores)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "Hwzy_8Rs3EV1"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = KGPL_Dataset.datasets['train']\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=4,  # optional for faster loading\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = KGPLStudent(model_cfg, 1872, 9366, 3846, train_dataset.adj_entity, train_dataset.adj_relation, train_dataset.path_list_dict, name='student').to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=model_cfg.optimize['lr'])\n",
        "\n",
        "# Train\n",
        "for epoch in tqdm(range(10)):\n",
        "    loss = train_one_epoch(model, train_loader, optimizer, device)\n",
        "    print(f\"Epoch {epoch} | Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkRFJM3d3EZw",
        "outputId": "af8c5508-d678-4ab7-92d6-8ff4bf60e095"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            " 10%|█         | 1/10 [00:05<00:50,  5.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 2.0796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:09<00:37,  4.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 2.0563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:13<00:30,  4.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Loss: 1.9670\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:17<00:25,  4.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Loss: 1.8977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:21<00:21,  4.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Loss: 1.8983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:25<00:16,  4.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Loss: 1.8561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [00:29<00:12,  4.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Loss: 1.8483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [00:34<00:08,  4.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Loss: 1.8483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [00:38<00:04,  4.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Loss: 1.8314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:41<00:00,  4.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Loss: 1.8124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mbiB_wXBP2Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_neighbors(self, seeds):\n",
        "        seeds = tf.expand_dims(seeds, axis=1)\n",
        "        entities = [seeds]\n",
        "        relations = []\n",
        "        for i in range(self.cfg.n_iter):\n",
        "            neighbor_entities = tf.reshape(\n",
        "                tf.gather(self.adj_entity, entities[i]), [self.batch_size, -1]\n",
        "            )\n",
        "            neighbor_relations = tf.reshape(\n",
        "                tf.gather(self.adj_relation, entities[i]), [self.batch_size, -1]\n",
        "            )\n",
        "            entities.append(neighbor_entities)\n",
        "            relations.append(neighbor_relations)\n",
        "        return entities, relations"
      ],
      "metadata": {
        "id": "rDBhuei8DNr7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}