{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nISq-HZfQx5",
        "outputId": "37d5583e-fc5b-45ef-9173-9dabcffd7a9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hydra-core==0.11.3 in /usr/local/lib/python3.11/dist-packages (0.11.3)\n",
            "Requirement already satisfied: omegaconf<1.5,>=1.4 in /usr/local/lib/python3.11/dist-packages (from hydra-core==0.11.3) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from omegaconf<1.5,>=1.4->hydra-core==0.11.3) (1.17.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from omegaconf<1.5,>=1.4->hydra-core==0.11.3) (6.0.2)\n",
            "Requirement already satisfied: omegaconf==1.4.1 in /usr/local/lib/python3.11/dist-packages (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from omegaconf==1.4.1) (1.17.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from omegaconf==1.4.1) (6.0.2)\n",
            "Requirement already satisfied: loguru==0.5.0 in /usr/local/lib/python3.11/dist-packages (0.5.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install hydra-core==0.11.3\n",
        "%pip install omegaconf==1.4.1\n",
        "%pip install loguru==0.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2fdXCTbWj2l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wfnXVLgSZG8",
        "outputId": "0e1f8e10-c9ee-49d5-96a8-d00b13f5cbc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'KGPL' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/riktor/KGPL/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyX4r9AgU1Md",
        "outputId": "46eaaab4-b11b-4ada-99aa-e7ece43a6a99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading item index to entity id file: data/movie/item_index2entity_id.txt ...\n",
            "reading rating file ...\n",
            "converting rating file ...\n",
            "number of users: 6040\n",
            "number of items: 2347\n",
            "converting kg file ...\n",
            "number of entities (containing items): 7008\n",
            "number of relations: 7\n",
            "done\n",
            "adj_entity_path: data/music/adj_entity_6_32.npy\n",
            "adj_relation_path: data/music/adj_relation_6_32.npy\n",
            "data_path: data/music/fold1.pkl\n",
            "dataset: music\n",
            "kg_path: data/music/kg_final.npy\n",
            "lp_depth: 6\n",
            "num_neighbor_samples: 32\n",
            "pathlist_path: data/music/path_list_6_32.pkl\n",
            "rating_path: data/music/ratings_final.npy\n",
            "reachable_items_path: data/music/reachable_items.pkl\n",
            "\n",
            "[Parallel(n_jobs=32)]: Using backend MultiprocessingBackend with 32 concurrent workers.\n",
            "[Parallel(n_jobs=32)]: Batch computation too fast (0.02765059471130371s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=32)]: Done   8 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=32)]: Done  21 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=32)]: Done  34 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=32)]: Done  50 tasks      | elapsed:    1.6s\n",
            "[Parallel(n_jobs=32)]: Done  70 tasks      | elapsed:    2.5s\n",
            "[Parallel(n_jobs=32)]: Batch computation too slow (2.022473650620264s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=32)]: Done 101 tasks      | elapsed:    3.7s\n",
            "[Parallel(n_jobs=32)]: Done 132 tasks      | elapsed:    5.1s\n",
            "[Parallel(n_jobs=32)]: Done 170 tasks      | elapsed:    5.7s\n",
            "[Parallel(n_jobs=32)]: Done 206 tasks      | elapsed:    6.9s\n",
            "[Parallel(n_jobs=32)]: Done 236 tasks      | elapsed:    8.0s\n",
            "[Parallel(n_jobs=32)]: Done 260 tasks      | elapsed:    8.6s\n",
            "[Parallel(n_jobs=32)]: Done 287 tasks      | elapsed:    9.9s\n",
            "[Parallel(n_jobs=32)]: Done 313 tasks      | elapsed:   11.1s\n",
            "[Parallel(n_jobs=32)]: Done 341 tasks      | elapsed:   11.8s\n",
            "[Parallel(n_jobs=32)]: Done 366 tasks      | elapsed:   12.5s\n",
            "[Parallel(n_jobs=32)]: Done 394 tasks      | elapsed:   13.6s\n",
            "[Parallel(n_jobs=32)]: Done 423 tasks      | elapsed:   14.8s\n",
            "[Parallel(n_jobs=32)]: Done 452 tasks      | elapsed:   15.7s\n",
            "[Parallel(n_jobs=32)]: Done 481 tasks      | elapsed:   16.9s\n",
            "[Parallel(n_jobs=32)]: Done 512 tasks      | elapsed:   18.0s\n",
            "[Parallel(n_jobs=32)]: Done 543 tasks      | elapsed:   19.7s\n",
            "[Parallel(n_jobs=32)]: Done 576 tasks      | elapsed:   21.0s\n",
            "[Parallel(n_jobs=32)]: Done 609 tasks      | elapsed:   22.2s\n",
            "[Parallel(n_jobs=32)]: Done 644 tasks      | elapsed:   23.8s\n",
            "[Parallel(n_jobs=32)]: Done 680 tasks      | elapsed:   25.5s\n",
            "[Parallel(n_jobs=32)]: Done 717 tasks      | elapsed:   26.1s\n",
            "[Parallel(n_jobs=32)]: Done 754 tasks      | elapsed:   27.6s\n",
            "[Parallel(n_jobs=32)]: Done 793 tasks      | elapsed:   30.1s\n",
            "[Parallel(n_jobs=32)]: Done 832 tasks      | elapsed:   32.0s\n",
            "[Parallel(n_jobs=32)]: Done 873 tasks      | elapsed:   33.0s\n",
            "[Parallel(n_jobs=32)]: Done 914 tasks      | elapsed:   34.5s\n",
            "[Parallel(n_jobs=32)]: Done 957 tasks      | elapsed:   35.9s\n",
            "[Parallel(n_jobs=32)]: Done 1000 tasks      | elapsed:   36.9s\n",
            "[Parallel(n_jobs=32)]: Done 1045 tasks      | elapsed:   38.2s\n",
            "[Parallel(n_jobs=32)]: Done 1090 tasks      | elapsed:   39.5s\n",
            "[Parallel(n_jobs=32)]: Done 1137 tasks      | elapsed:   41.1s\n",
            "[Parallel(n_jobs=32)]: Done 1184 tasks      | elapsed:   42.8s\n",
            "[Parallel(n_jobs=32)]: Done 1233 tasks      | elapsed:   44.1s\n",
            "[Parallel(n_jobs=32)]: Done 1282 tasks      | elapsed:   46.0s\n",
            "[Parallel(n_jobs=32)]: Done 1333 tasks      | elapsed:   48.3s\n",
            "[Parallel(n_jobs=32)]: Done 1384 tasks      | elapsed:   50.2s\n",
            "[Parallel(n_jobs=32)]: Done 1437 tasks      | elapsed:   52.1s\n",
            "[Parallel(n_jobs=32)]: Done 1490 tasks      | elapsed:   54.2s\n",
            "[Parallel(n_jobs=32)]: Done 1545 tasks      | elapsed:   55.9s\n",
            "[Parallel(n_jobs=32)]: Done 1600 tasks      | elapsed:   58.1s\n",
            "[Parallel(n_jobs=32)]: Done 1657 tasks      | elapsed:  1.0min\n",
            "[Parallel(n_jobs=32)]: Done 1714 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=32)]: Done 1773 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=32)]: Done 1832 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=32)]: Done 1893 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=32)]: Done 1954 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=32)]: Done 2017 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=32)]: Done 2080 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=32)]: Done 2145 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=32)]: Done 2210 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=32)]: Done 2277 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=32)]: Done 2344 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=32)]: Done 2413 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=32)]: Done 2482 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=32)]: Done 2553 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=32)]: Done 2624 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=32)]: Done 2697 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=32)]: Done 2770 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=32)]: Done 2845 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=32)]: Done 2920 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=32)]: Done 2997 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=32)]: Done 3074 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=32)]: Done 3153 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=32)]: Done 3232 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=32)]: Done 3313 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=32)]: Done 3394 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=32)]: Done 3477 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=32)]: Done 3560 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=32)]: Done 3645 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=32)]: Done 3730 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=32)]: Done 3846 out of 3846 | elapsed:  2.4min finished\n",
            "average number of paths: 530.5631825273011\n",
            "median number of paths: 247.0\n",
            "min number of paths: 2\n",
            "max number of paths: 6636\n",
            "reading item index to entity id file: data/music/item_index2entity_id.txt ...\n",
            "reading rating file ...\n",
            "converting rating file ...\n",
            "number of users: 1872\n",
            "number of items: 3846\n",
            "converting kg file ...\n",
            "number of entities (containing items): 9366\n",
            "number of relations: 60\n",
            "done\n",
            "adj_entity_path: data/music/adj_entity_6_32.npy\n",
            "adj_relation_path: data/music/adj_relation_6_32.npy\n",
            "data_path: data/music/fold1.pkl\n",
            "dataset: music\n",
            "kg_path: data/music/kg_final.npy\n",
            "lp_depth: 6\n",
            "num_neighbor_samples: 32\n",
            "pathlist_path: data/music/path_list_6_32.pkl\n",
            "rating_path: data/music/ratings_final.npy\n",
            "reachable_items_path: data/music/reachable_items.pkl\n",
            "\n",
            "[Parallel(n_jobs=32)]: Using backend MultiprocessingBackend with 32 concurrent workers.\n",
            "[Parallel(n_jobs=32)]: Batch computation too fast (0.005289316177368164s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=32)]: Done   8 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=32)]: Done  21 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=32)]: Done  34 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=32)]: Done  50 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=32)]: Done  70 tasks      | elapsed:    2.6s\n",
            "[Parallel(n_jobs=32)]: Batch computation too slow (2.0054991631035164s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=32)]: Done 102 tasks      | elapsed:    4.0s\n",
            "[Parallel(n_jobs=32)]: Done 133 tasks      | elapsed:    5.1s\n",
            "[Parallel(n_jobs=32)]: Done 171 tasks      | elapsed:    6.5s\n",
            "[Parallel(n_jobs=32)]: Done 205 tasks      | elapsed:    7.2s\n",
            "[Parallel(n_jobs=32)]: Done 235 tasks      | elapsed:    8.2s\n",
            "[Parallel(n_jobs=32)]: Done 259 tasks      | elapsed:    9.1s\n",
            "[Parallel(n_jobs=32)]: Done 288 tasks      | elapsed:    9.8s\n",
            "[Parallel(n_jobs=32)]: Done 314 tasks      | elapsed:   10.7s\n",
            "[Parallel(n_jobs=32)]: Done 339 tasks      | elapsed:   11.5s\n",
            "[Parallel(n_jobs=32)]: Done 366 tasks      | elapsed:   12.1s\n",
            "[Parallel(n_jobs=32)]: Done 393 tasks      | elapsed:   13.2s\n",
            "[Parallel(n_jobs=32)]: Done 422 tasks      | elapsed:   14.7s\n",
            "[Parallel(n_jobs=32)]: Done 451 tasks      | elapsed:   15.7s\n",
            "[Parallel(n_jobs=32)]: Done 480 tasks      | elapsed:   17.0s\n",
            "[Parallel(n_jobs=32)]: Done 511 tasks      | elapsed:   18.2s\n",
            "[Parallel(n_jobs=32)]: Done 542 tasks      | elapsed:   19.5s\n",
            "[Parallel(n_jobs=32)]: Done 575 tasks      | elapsed:   20.7s\n",
            "[Parallel(n_jobs=32)]: Done 609 tasks      | elapsed:   22.0s\n",
            "[Parallel(n_jobs=32)]: Done 644 tasks      | elapsed:   23.3s\n",
            "[Parallel(n_jobs=32)]: Done 679 tasks      | elapsed:   24.9s\n",
            "[Parallel(n_jobs=32)]: Done 716 tasks      | elapsed:   25.7s\n",
            "[Parallel(n_jobs=32)]: Done 753 tasks      | elapsed:   26.8s\n",
            "[Parallel(n_jobs=32)]: Done 793 tasks      | elapsed:   29.0s\n",
            "[Parallel(n_jobs=32)]: Done 832 tasks      | elapsed:   30.6s\n",
            "[Parallel(n_jobs=32)]: Done 873 tasks      | elapsed:   31.8s\n",
            "[Parallel(n_jobs=32)]: Done 914 tasks      | elapsed:   33.6s\n",
            "[Parallel(n_jobs=32)]: Done 957 tasks      | elapsed:   35.3s\n",
            "[Parallel(n_jobs=32)]: Done 1000 tasks      | elapsed:   36.7s\n",
            "[Parallel(n_jobs=32)]: Done 1045 tasks      | elapsed:   37.7s\n",
            "[Parallel(n_jobs=32)]: Done 1090 tasks      | elapsed:   38.8s\n",
            "[Parallel(n_jobs=32)]: Done 1137 tasks      | elapsed:   40.6s\n",
            "[Parallel(n_jobs=32)]: Done 1184 tasks      | elapsed:   42.5s\n",
            "[Parallel(n_jobs=32)]: Done 1233 tasks      | elapsed:   43.5s\n",
            "[Parallel(n_jobs=32)]: Done 1282 tasks      | elapsed:   45.6s\n",
            "[Parallel(n_jobs=32)]: Done 1333 tasks      | elapsed:   48.5s\n",
            "[Parallel(n_jobs=32)]: Done 1384 tasks      | elapsed:   50.7s\n",
            "[Parallel(n_jobs=32)]: Done 1437 tasks      | elapsed:   52.2s\n",
            "[Parallel(n_jobs=32)]: Done 1490 tasks      | elapsed:   54.5s\n",
            "[Parallel(n_jobs=32)]: Done 1545 tasks      | elapsed:   56.3s\n",
            "[Parallel(n_jobs=32)]: Done 1600 tasks      | elapsed:   58.7s\n",
            "[Parallel(n_jobs=32)]: Done 1657 tasks      | elapsed:  1.0min\n",
            "[Parallel(n_jobs=32)]: Done 1714 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=32)]: Done 1773 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=32)]: Done 1832 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=32)]: Done 1893 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=32)]: Done 1954 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=32)]: Done 2017 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=32)]: Done 2080 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=32)]: Done 2145 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=32)]: Done 2210 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=32)]: Done 2277 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=32)]: Done 2344 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=32)]: Done 2413 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=32)]: Done 2482 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=32)]: Done 2553 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=32)]: Done 2624 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=32)]: Done 2697 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=32)]: Done 2770 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=32)]: Done 2845 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=32)]: Done 2920 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=32)]: Done 2997 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=32)]: Done 3074 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=32)]: Done 3153 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=32)]: Done 3232 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=32)]: Done 3313 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=32)]: Done 3394 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=32)]: Done 3477 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=32)]: Done 3560 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=32)]: Done 3645 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=32)]: Done 3730 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=32)]: Done 3846 out of 3846 | elapsed:  2.4min finished\n",
            "average number of paths: 549.106864274571\n",
            "median number of paths: 276.5\n",
            "min number of paths: 2\n",
            "max number of paths: 9913\n",
            "reading item index to entity id file: data/book/item_index2entity_id.txt ...\n",
            "reading rating file ...\n",
            "converting rating file ...\n",
            "number of users: 17860\n",
            "number of items: 14967\n",
            "converting kg file ...\n",
            "number of entities (containing items): 77903\n",
            "number of relations: 25\n",
            "done\n",
            "adj_entity_path: data/book/adj_entity_6_32.npy\n",
            "adj_relation_path: data/book/adj_relation_6_32.npy\n",
            "data_path: data/book/fold1.pkl\n",
            "dataset: book\n",
            "kg_path: data/book/kg_final.npy\n",
            "lp_depth: 6\n",
            "num_neighbor_samples: 8\n",
            "pathlist_path: data/book/path_list_6_32.pkl\n",
            "rating_path: data/book/ratings_final.npy\n",
            "reachable_items_path: data/book/reachable_items.pkl\n",
            "\n",
            "[Parallel(n_jobs=32)]: Using backend MultiprocessingBackend with 32 concurrent workers.\n",
            "[Parallel(n_jobs=32)]: Batch computation too fast (0.014830827713012695s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=32)]: Done   8 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=32)]: Done  21 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=32)]: Done  34 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=32)]: Done  51 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=32)]: Done  73 tasks      | elapsed:    1.4s\n",
            "[Parallel(n_jobs=32)]: Done 101 tasks      | elapsed:    1.9s\n",
            "[Parallel(n_jobs=32)]: Done 134 tasks      | elapsed:    2.5s\n",
            "[Parallel(n_jobs=32)]: Batch computation too slow (2.0099481643881196s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=32)]: Done 170 tasks      | elapsed:    3.1s\n",
            "[Parallel(n_jobs=32)]: Done 208 tasks      | elapsed:    3.9s\n",
            "[Parallel(n_jobs=32)]: Done 250 tasks      | elapsed:    4.8s\n",
            "[Parallel(n_jobs=32)]: Done 285 tasks      | elapsed:    5.5s\n",
            "[Parallel(n_jobs=32)]: Done 319 tasks      | elapsed:    6.4s\n",
            "[Parallel(n_jobs=32)]: Done 347 tasks      | elapsed:    7.0s\n",
            "[Parallel(n_jobs=32)]: Done 375 tasks      | elapsed:    7.6s\n",
            "[Parallel(n_jobs=32)]: Done 402 tasks      | elapsed:    8.0s\n",
            "[Parallel(n_jobs=32)]: Done 429 tasks      | elapsed:    8.5s\n",
            "[Parallel(n_jobs=32)]: Done 456 tasks      | elapsed:    9.0s\n",
            "[Parallel(n_jobs=32)]: Done 485 tasks      | elapsed:    9.7s\n",
            "[Parallel(n_jobs=32)]: Done 514 tasks      | elapsed:   10.2s\n",
            "[Parallel(n_jobs=32)]: Done 545 tasks      | elapsed:   10.7s\n",
            "[Parallel(n_jobs=32)]: Done 576 tasks      | elapsed:   11.3s\n",
            "[Parallel(n_jobs=32)]: Done 609 tasks      | elapsed:   12.3s\n",
            "[Parallel(n_jobs=32)]: Done 642 tasks      | elapsed:   13.0s\n",
            "[Parallel(n_jobs=32)]: Done 677 tasks      | elapsed:   13.6s\n",
            "[Parallel(n_jobs=32)]: Done 712 tasks      | elapsed:   14.3s\n",
            "[Parallel(n_jobs=32)]: Done 749 tasks      | elapsed:   15.1s\n",
            "[Parallel(n_jobs=32)]: Done 786 tasks      | elapsed:   15.6s\n",
            "[Parallel(n_jobs=32)]: Done 825 tasks      | elapsed:   16.1s\n",
            "[Parallel(n_jobs=32)]: Done 864 tasks      | elapsed:   16.8s\n",
            "[Parallel(n_jobs=32)]: Done 905 tasks      | elapsed:   17.6s\n",
            "[Parallel(n_jobs=32)]: Done 946 tasks      | elapsed:   18.1s\n",
            "[Parallel(n_jobs=32)]: Done 989 tasks      | elapsed:   19.0s\n",
            "[Parallel(n_jobs=32)]: Done 1032 tasks      | elapsed:   19.6s\n",
            "[Parallel(n_jobs=32)]: Done 1077 tasks      | elapsed:   20.3s\n",
            "[Parallel(n_jobs=32)]: Done 1122 tasks      | elapsed:   21.2s\n",
            "[Parallel(n_jobs=32)]: Done 1169 tasks      | elapsed:   22.0s\n",
            "[Parallel(n_jobs=32)]: Done 1216 tasks      | elapsed:   22.6s\n",
            "[Parallel(n_jobs=32)]: Done 1265 tasks      | elapsed:   23.4s\n",
            "[Parallel(n_jobs=32)]: Done 1314 tasks      | elapsed:   24.4s\n",
            "[Parallel(n_jobs=32)]: Done 1365 tasks      | elapsed:   25.3s\n",
            "[Parallel(n_jobs=32)]: Done 1416 tasks      | elapsed:   26.3s\n",
            "[Parallel(n_jobs=32)]: Done 1469 tasks      | elapsed:   27.2s\n",
            "[Parallel(n_jobs=32)]: Done 1522 tasks      | elapsed:   28.0s\n",
            "[Parallel(n_jobs=32)]: Done 1577 tasks      | elapsed:   28.7s\n",
            "[Parallel(n_jobs=32)]: Done 1632 tasks      | elapsed:   29.9s\n",
            "[Parallel(n_jobs=32)]: Done 1689 tasks      | elapsed:   31.0s\n",
            "[Parallel(n_jobs=32)]: Done 1746 tasks      | elapsed:   32.2s\n",
            "[Parallel(n_jobs=32)]: Done 1805 tasks      | elapsed:   33.0s\n",
            "[Parallel(n_jobs=32)]: Done 1864 tasks      | elapsed:   34.2s\n",
            "[Parallel(n_jobs=32)]: Done 1925 tasks      | elapsed:   35.2s\n",
            "[Parallel(n_jobs=32)]: Done 1986 tasks      | elapsed:   36.8s\n",
            "[Parallel(n_jobs=32)]: Done 2049 tasks      | elapsed:   38.2s\n",
            "[Parallel(n_jobs=32)]: Done 2112 tasks      | elapsed:   38.9s\n",
            "[Parallel(n_jobs=32)]: Done 2177 tasks      | elapsed:   39.8s\n",
            "[Parallel(n_jobs=32)]: Done 2242 tasks      | elapsed:   40.5s\n",
            "[Parallel(n_jobs=32)]: Done 2309 tasks      | elapsed:   41.2s\n",
            "[Parallel(n_jobs=32)]: Done 2376 tasks      | elapsed:   42.1s\n",
            "[Parallel(n_jobs=32)]: Done 2445 tasks      | elapsed:   43.3s\n",
            "[Parallel(n_jobs=32)]: Done 2514 tasks      | elapsed:   44.5s\n",
            "[Parallel(n_jobs=32)]: Done 2585 tasks      | elapsed:   45.7s\n",
            "[Parallel(n_jobs=32)]: Done 2656 tasks      | elapsed:   46.7s\n",
            "[Parallel(n_jobs=32)]: Done 2729 tasks      | elapsed:   48.1s\n",
            "[Parallel(n_jobs=32)]: Done 2802 tasks      | elapsed:   49.3s\n",
            "[Parallel(n_jobs=32)]: Done 2877 tasks      | elapsed:   50.5s\n",
            "[Parallel(n_jobs=32)]: Done 2952 tasks      | elapsed:   52.2s\n",
            "[Parallel(n_jobs=32)]: Done 3029 tasks      | elapsed:   53.4s\n",
            "[Parallel(n_jobs=32)]: Done 3106 tasks      | elapsed:   54.8s\n",
            "[Parallel(n_jobs=32)]: Done 3185 tasks      | elapsed:   56.4s\n",
            "[Parallel(n_jobs=32)]: Done 3264 tasks      | elapsed:   57.5s\n",
            "[Parallel(n_jobs=32)]: Done 3345 tasks      | elapsed:   58.5s\n",
            "[Parallel(n_jobs=32)]: Done 3426 tasks      | elapsed:   59.9s\n",
            "[Parallel(n_jobs=32)]: Done 3509 tasks      | elapsed:  1.0min\n",
            "[Parallel(n_jobs=32)]: Done 3592 tasks      | elapsed:  1.0min\n",
            "[Parallel(n_jobs=32)]: Done 3677 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=32)]: Done 3762 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=32)]: Done 3849 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=32)]: Done 3936 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=32)]: Done 4025 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=32)]: Done 4114 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=32)]: Done 4205 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=32)]: Done 4296 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=32)]: Done 4389 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=32)]: Done 4482 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=32)]: Done 4577 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=32)]: Done 4672 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=32)]: Done 4769 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=32)]: Done 4866 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=32)]: Done 4965 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=32)]: Done 5064 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=32)]: Done 5165 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=32)]: Done 5266 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=32)]: Done 5369 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=32)]: Done 5472 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=32)]: Done 5577 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=32)]: Done 5682 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=32)]: Done 5789 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=32)]: Done 5896 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=32)]: Done 6005 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=32)]: Done 6114 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=32)]: Done 6225 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=32)]: Done 6336 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=32)]: Done 6449 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=32)]: Done 6562 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=32)]: Done 6677 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=32)]: Done 6792 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=32)]: Done 6909 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=32)]: Done 7026 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=32)]: Done 7145 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=32)]: Done 7264 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=32)]: Done 7385 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=32)]: Done 7506 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=32)]: Done 7629 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=32)]: Done 7752 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=32)]: Done 7877 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=32)]: Done 8002 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=32)]: Done 8129 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=32)]: Done 8256 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=32)]: Done 8385 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=32)]: Done 8514 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=32)]: Done 8645 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=32)]: Done 8776 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=32)]: Done 8909 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=32)]: Done 9042 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=32)]: Done 9177 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=32)]: Done 9312 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=32)]: Done 9449 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=32)]: Done 9586 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=32)]: Done 9725 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=32)]: Done 9864 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=32)]: Done 10005 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=32)]: Done 10146 tasks      | elapsed:  2.8min\n",
            "[Parallel(n_jobs=32)]: Done 10289 tasks      | elapsed:  2.8min\n",
            "[Parallel(n_jobs=32)]: Done 10432 tasks      | elapsed:  2.8min\n",
            "[Parallel(n_jobs=32)]: Done 10577 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs=32)]: Done 10722 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs=32)]: Done 10869 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs=32)]: Done 11016 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=32)]: Done 11165 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=32)]: Done 11314 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=32)]: Done 11465 tasks      | elapsed:  3.1min\n",
            "[Parallel(n_jobs=32)]: Done 11616 tasks      | elapsed:  3.1min\n",
            "[Parallel(n_jobs=32)]: Done 11769 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=32)]: Done 11922 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=32)]: Done 12077 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=32)]: Done 12232 tasks      | elapsed:  3.3min\n",
            "[Parallel(n_jobs=32)]: Done 12389 tasks      | elapsed:  3.3min\n",
            "[Parallel(n_jobs=32)]: Done 12546 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=32)]: Done 12705 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=32)]: Done 12864 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=32)]: Done 13025 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=32)]: Done 13186 tasks      | elapsed:  3.5min\n",
            "[Parallel(n_jobs=32)]: Done 13349 tasks      | elapsed:  3.5min\n",
            "[Parallel(n_jobs=32)]: Done 13512 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=32)]: Done 13677 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=32)]: Done 13842 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=32)]: Done 14009 tasks      | elapsed:  3.7min\n",
            "[Parallel(n_jobs=32)]: Done 14176 tasks      | elapsed:  3.7min\n",
            "[Parallel(n_jobs=32)]: Done 14345 tasks      | elapsed:  3.8min\n",
            "[Parallel(n_jobs=32)]: Done 14514 tasks      | elapsed:  3.8min\n",
            "[Parallel(n_jobs=32)]: Done 14685 tasks      | elapsed:  3.9min\n",
            "[Parallel(n_jobs=32)]: Done 14856 tasks      | elapsed:  3.9min\n",
            "[Parallel(n_jobs=32)]: Done 14966 out of 14966 | elapsed:  3.9min finished\n",
            "average number of paths: 25.263797942001872\n",
            "median number of paths: 14.0\n",
            "min number of paths: 0\n",
            "max number of paths: 295\n"
          ]
        }
      ],
      "source": [
        "!cd KGPL &&\\\n",
        "python preprocess/preprocess.py -d \"movie\" &&\\\n",
        "python preprocess/make_path_list.py lp_depth=6 dataset=music kg_path=data/music/kg_final.npy rating_path=data/music/ratings_final.npy num_neighbor_samples=32\n",
        "\n",
        "!cd KGPL &&\\\n",
        "python preprocess/preprocess.py -d \"music\" &&\\\n",
        "python preprocess/make_path_list.py lp_depth=6 dataset=music kg_path=data/music/kg_final.npy rating_path=data/music/ratings_final.npy num_neighbor_samples=32\n",
        "\n",
        "!cd KGPL &&\\\n",
        "python preprocess/preprocess.py -d \"book\" &&\\\n",
        "python preprocess/make_path_list.py lp_depth=6 dataset=book kg_path=data/book/kg_final.npy rating_path=data/book/ratings_final.npy num_neighbor_samples=8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQlrACnVm6ko"
      },
      "outputs": [],
      "source": [
        "# from KGPL.models.kgpl import KGPL_COT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcojKL0Fpii5",
        "outputId": "0c0ae223-ee4c-4fc3-d6bf-2d188e9f4679"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([1., 2., 3.]), tensor([1, 1, 1]))"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.unique(torch.Tensor([1,2,3]), return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "UhZOnz55y5tT"
      },
      "outputs": [],
      "source": [
        "class KGPL_Config():\n",
        "  '''\n",
        "  KGPL model configuration for each dataset.\n",
        "  '''\n",
        "  def __init__(self, dataset_name:str, model_type:str, neighbor_sample_size:int, dropout_rate:float, emb_dim:int, n_iter:int, plabel:dict, optimize:dict, log:dict, evaluate:dict, model:dict):\n",
        "    self.dataset_name = dataset_name\n",
        "    self.model_type = model_type\n",
        "    self.neighbor_sample_size = neighbor_sample_size\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.emb_dim = emb_dim\n",
        "    self.n_iter = n_iter\n",
        "    self.plabel = plabel\n",
        "    self.optimize = optimize\n",
        "    self.log = log\n",
        "    self.evaluate = evaluate\n",
        "    self.model = model\n",
        "\n",
        "\n",
        "\n",
        "def build_user_train_dict_from_tensor(ratings):\n",
        "    user_train = {}\n",
        "    for i in range(ratings.size(0)):\n",
        "        user, item, rating = ratings[i]\n",
        "        if rating >= 1:  # or whatever you define as \"positive\" interaction\n",
        "            user = user.item()\n",
        "            item = item.item()\n",
        "            if user not in user_train:\n",
        "                user_train[user] = []\n",
        "            user_train[user].append(item)\n",
        "    return user_train\n",
        "\n",
        "# def compute_reachable_items_torch(args_list):\n",
        "def compute_reachable_items_nodist(seed_items, kg):\n",
        "    source = kg[:,0]\n",
        "    target = kg[:,2]\n",
        "    # print(source)\n",
        "    # print(target)\n",
        "    mask = (source.unsqueeze(1) == seed_items).any(dim=1) | (target.unsqueeze(1) == seed_items).any(dim=1)\n",
        "    connected_edges = kg[mask]\n",
        "    neighbors = torch.cat([connected_edges[:, 0], connected_edges[:, 2]])\n",
        "    neighbors = neighbors[~torch.isin(neighbors, seed_items)]\n",
        "    return torch.unique(neighbors)\n",
        "\n",
        "\n",
        "def compute_reachable_items_torch(args_list):\n",
        "    idd = {}\n",
        "    _, _, dst_dict, item_freq, pn = args_list[0]\n",
        "    for args in args_list:\n",
        "        if args is None:\n",
        "            continue\n",
        "        user, seed_items, _, _, _ = args\n",
        "\n",
        "        dst = Counter()\n",
        "        for item in seed_items:\n",
        "            if item in dst_dict:\n",
        "                dst += dst_dict[item]\n",
        "\n",
        "        if len(dst) != 0:\n",
        "            udst = torch.tensor(list(dst.keys()), dtype=torch.long)\n",
        "            F = torch.tensor(list(dst.values()), dtype=torch.float) ** pn\n",
        "\n",
        "            mask = ~torch.isin(udst, torch.tensor(list(seed_items), dtype=torch.long))\n",
        "            udst = udst[mask]\n",
        "            F = F[mask]\n",
        "\n",
        "            udst_set = set(udst.tolist())\n",
        "            unreachable_items = [i for i in item_freq if i not in udst_set]\n",
        "            udst = torch.cat([udst, torch.tensor(unreachable_items, dtype=torch.long)])\n",
        "\n",
        "            F = torch.cat([F, torch.ones(len(unreachable_items)) * 0.5])\n",
        "\n",
        "            sort_inds = torch.argsort(F)\n",
        "            udst = udst[sort_inds]\n",
        "            F = F[sort_inds]\n",
        "\n",
        "            F = F / F.sum()\n",
        "            F = torch.cumsum(F, dim=0)\n",
        "\n",
        "            idd[user] = (udst, F)\n",
        "    return idd\n",
        "\n",
        "def set_item_candidates(\n",
        "        self, n_user, n_item, train_data, eval_data, path_list_dict\n",
        "    ):\n",
        "        \"\"\"Construct the sampling distrbiutions for negative/pseudo-labelled instances for each user\n",
        "        \"\"\"\n",
        "        all_users = tuple(set(train_data[:, 0]))\n",
        "        self.all_users = all_users\n",
        "\n",
        "        self.n_item = n_item\n",
        "        self.all_items = set(range(n_item))\n",
        "        self.neg_c_dict_user = self._build_freq_dict(\n",
        "            np.concatenate([train_data[:, 0], eval_data[:, 0]]), self.all_users\n",
        "        )\n",
        "        self.neg_c_dict_item = self._build_freq_dict(\n",
        "            np.concatenate([train_data[:, 1], eval_data[:, 1]]), self.all_items\n",
        "        )\n",
        "\n",
        "        item_cands = tuple(self.neg_c_dict_item.keys())\n",
        "        F = np.array(tuple(self.neg_c_dict_item.values())) ** self.cfg.plabel.neg_pn\n",
        "        sort_inds = np.argsort(F)\n",
        "        item_cands = [item_cands[i] for i in sort_inds]\n",
        "        F = F[sort_inds]\n",
        "        F = (F / F.sum()).cumsum()\n",
        "        self.item_freq = (item_cands, F)\n",
        "\n",
        "        for u, i in tqdm(train_data[:, 0:2]):\n",
        "            self.user_seed_dict[u].add(i)\n",
        "\n",
        "        path = hydra.utils.to_absolute_path(self.cfg.reachable_items_path)\n",
        "        logger.info(\"calculating reachable items for users\")\n",
        "        self._setup_dst_dict(path_list_dict)\n",
        "        item_dist_dict = {}\n",
        "        src_itr = map(\n",
        "            lambda iu: (\n",
        "                all_users[iu],\n",
        "                tuple(self.user_seed_dict[all_users[iu]]),\n",
        "                self.dst_dict,\n",
        "                self.neg_c_dict_item,\n",
        "                self.cfg.plabel.pl_pn,\n",
        "            ),\n",
        "            range(len(all_users)),\n",
        "        )\n",
        "        grouped = grouper(self.cfg.plabel.chunk_size, src_itr, squash=set([2, 3]))\n",
        "        with mp.Pool(self.cfg.plabel.par) as pool:\n",
        "            for idd in pool.imap_unordered(compute_reachable_items_, grouped):\n",
        "                item_dist_dict.update(idd)\n",
        "        self.item_dist_dict = item_dist_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "PPnbQROj5vLV"
      },
      "outputs": [],
      "source": [
        "# def compute_reachable_items(ratings):\n",
        "#     \"\"\"\n",
        "#     ratings: torch.Tensor of shape (N, 3), where each row is (user, item, rating)\n",
        "#     \"\"\"\n",
        "#     reachable_items = dict()\n",
        "\n",
        "#     for user, item, rating in ratings.tolist():\n",
        "#         if rating == 1:  # positive interaction\n",
        "#             if user not in reachable_items:\n",
        "#                 reachable_items[user.item()] = set()\n",
        "#             reachable_items[user.item()].add(item.item())\n",
        "\n",
        "#     return reachable_items\n",
        "\n",
        "def compute_reachable_items(users, user_seed_dict, path_dict, item_freq, power):\n",
        "    \"\"\"\n",
        "    For each user, find reachable items via KG paths and compute a pseudo-label distribution.\n",
        "    This is adapted from compute_reachable_items_.\n",
        "    Returns: dict of user -> (item_list, cumulative_probs)\n",
        "    \"\"\"\n",
        "    reachable = {}\n",
        "    for user in users:\n",
        "        seed_items = user_seed_dict.get(user, [])\n",
        "        # Count reachable paths from each seed\n",
        "        dst = Counter()\n",
        "        for item in seed_items:\n",
        "            dst.update(path_dict.get(item, {}))\n",
        "        if not dst:\n",
        "            continue\n",
        "        udst = np.array(list(dst.keys()))\n",
        "        freq = np.array(list(dst.values())) ** power\n",
        "        # Remove seeds from candidates\n",
        "        mask = ~np.isin(udst, list(seed_items))\n",
        "        udst = udst[mask]\n",
        "        freq = freq[mask]\n",
        "        # Add 'unreachable' items with small weight\n",
        "        all_items = np.array(list(item_freq.keys()))\n",
        "        unreachable = np.setdiff1d(all_items, udst, assume_unique=True)\n",
        "        freq = np.concatenate([freq, np.ones(len(unreachable)) * 0.5])\n",
        "        udst = np.concatenate([udst, unreachable])\n",
        "        # Sort and make CDF\n",
        "        order = np.argsort(freq)\n",
        "        udst = udst[order]; freq = freq[order]\n",
        "        cdf = (freq / freq.sum()).cumsum()\n",
        "        reachable[user] = (udst.tolist(), cdf.tolist())\n",
        "    return reachable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "yO2G4T1P76uG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "I0kfcAmR73FM",
        "outputId": "fe742368-4876-465c-b59c-35d268c72973"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'self' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-251-833b4f6db1c7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_seed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ],
      "source": [
        "for u, i in train_data[:, 0:2]:\n",
        "    self.user_seed_dict[u].add(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "kb4mI82_TQZ3"
      },
      "outputs": [],
      "source": [
        "class KGPL_Dataset(Dataset):\n",
        "  '''\n",
        "  Custom dataset class which includes all datasets and parameters per model.\n",
        "  Specified under \"data\" directory\n",
        "  '''\n",
        "\n",
        "  base_data_path = 'KGPL/data/'\n",
        "\n",
        "  def readjust_counts(self):\n",
        "    unique_users = torch.unique(self.ratings[:,0], return_counts=True)\n",
        "    self.users = unique_users[0]\n",
        "    self.n_user = unique_users[1][0].item()\n",
        "    self.n_item = torch.unique(self.ratings[:,1]).numel()\n",
        "    self.reachable_items = compute_reachable_items_nodist(self.users, self.ratings)\n",
        "\n",
        "  def __init__(self,dataset_name:str):\n",
        "    self.dataset_name = dataset_name\n",
        "    self.entity_adj = torch.from_numpy(np.load(self.base_data_path + self.dataset_name + '/adj_entity_6_32.npy'))\n",
        "    self.relation_adj = torch.from_numpy(np.load(self.base_data_path + self.dataset_name + '/adj_relation_6_32.npy'))\n",
        "    self.ratings = torch.from_numpy(np.load(self.base_data_path + self.dataset_name + '/ratings_final.npy'))\n",
        "    self.path_list_dict = pickle.load(open(self.base_data_path + self.dataset_name + '/path_list_6_32.pkl', 'rb'))\n",
        "    #kg is a 3-column matrix of undirected relations: (head, relation, tail)\n",
        "    self.kg = np.load(self.base_data_path + self.dataset_name + '/kg_final.npy')\n",
        "    self.readjust_counts()\n",
        "\n",
        "  def __len__(self):\n",
        "    # doing interactions\n",
        "    return self.n_user\n",
        "\n",
        "  def sample_positive(self, user):\n",
        "        return random.choice(self.user_train[user])\n",
        "\n",
        "  def sample_negative(self, user):\n",
        "      seen = set(self.user_train[user])\n",
        "      while True:\n",
        "          item = random.randint(0, self.n_item - 1)\n",
        "          if item not in seen:\n",
        "              return item\n",
        "\n",
        "  # def sample_pseudo_label(self, user):\n",
        "  #       F = compute_reachable_items_torch(args_list)\n",
        "  #       # udst, F = self.reachable_items[user]\n",
        "  #       r = random.random()\n",
        "  #       idx = torch.searchsorted(F, r, right=True).item()\n",
        "  #       return udst[idx].item()\n",
        "  def sample_pseudo_label(self, user):\n",
        "        udst, F = self.reachable_items[user]\n",
        "        r = random.random()\n",
        "        idx = torch.searchsorted(F, r, right=True).item()\n",
        "        return udst[idx].item()\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    if self.train_set:\n",
        "      user = self.users[idx]\n",
        "      pos_item = self.sample_positive(idx)\n",
        "      neg_item = self.sample_negative(idx)\n",
        "      pseudo_label = self.sample_pseudo_label(user)\n",
        "      return user, pos_item, neg_item, pseudo_label\n",
        "    else:\n",
        "      # still need to implement\n",
        "      return None\n",
        "\n",
        "  def _split_data(self, split_ratio=0.2):\n",
        "    #split dataset\n",
        "    n_ratings = len(self.ratings)\n",
        "    split_indices = torch.randperm(n_ratings)[:int(n_ratings * split_ratio)]\n",
        "    splitted_data = self.ratings[split_indices]\n",
        "    rest_data = self.ratings[~torch.isin(torch.arange(n_ratings), split_indices)]\n",
        "    #create new objects\n",
        "    splitted_dataset, rest_dataset = deepcopy(self), deepcopy(self)\n",
        "    splitted_dataset.ratings = splitted_data\n",
        "    rest_dataset.ratings = rest_data\n",
        "    splitted_dataset.readjust_counts()\n",
        "    rest_dataset.readjust_counts()\n",
        "\n",
        "    return rest_dataset, splitted_dataset\n",
        "\n",
        "  def train_val_test_split(self):\n",
        "    exp_dataset, test = self._split_data()\n",
        "    train, val = exp_dataset._split_data()\n",
        "    n_user = torch.unique(self.ratings[:,0]).numel()\n",
        "    n_item = torch.unique(self.ratings[:,1]).numel()\n",
        "    #readjust counts\n",
        "    train.readjust_counts()\n",
        "    val.readjust_counts()\n",
        "    test.readjust_counts()\n",
        "    train.user_train = build_user_train_dict_from_tensor(train.ratings)\n",
        "    train.train_set = True\n",
        "    return (n_user, n_item, train, val, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw0SB_VMHkKC"
      },
      "outputs": [],
      "source": [
        "data = KGPL_Dataset('music').train_val_test_split()\n",
        "\n",
        "cfg = KGPL_Config(\n",
        "    'music',\n",
        "    'KGPL_COT',\n",
        "    neighbor_sample_size=32,\n",
        "    dropout_rate=0.5,\n",
        "    emb_dim=64,\n",
        "    n_iter=1,\n",
        "    plabel={},\n",
        "    optimize={'iter_per_epoch':100, 'lr': 3e-3, 'batch_size':3333},\n",
        "    log={'show_loss':True},\n",
        "    evaluate={'user_num_topk':1000},\n",
        "    model={'n_iter':1, 'neighbor_sample_size':32, 'dropout_rate':0.5}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "CK8U4-vhmFlX"
      },
      "outputs": [],
      "source": [
        "def kgpl_loss(pos_scores, neg_scores, pseudo_scores):\n",
        "    # BCE loss like TensorFlow version\n",
        "    pos_labels = torch.ones_like(pos_scores)\n",
        "    neg_labels = torch.zeros_like(neg_scores)\n",
        "    pseudo_labels = torch.ones_like(pseudo_scores)\n",
        "\n",
        "    loss = F.binary_cross_entropy_with_logits(pos_scores, pos_labels) + \\\n",
        "           F.binary_cross_entropy_with_logits(neg_scores, neg_labels) + \\\n",
        "           F.binary_cross_entropy_with_logits(pseudo_scores, pseudo_labels)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "MJWzXvdwmFgA"
      },
      "outputs": [],
      "source": [
        "class SumAggregatorWithDropout(nn.Module):\n",
        "    def __init__(self, emb_dim, dropout_rate, activation, cfg):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(emb_dim * 2, emb_dim)\n",
        "        self.activation = activation\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, self_vectors, neighbor_vectors, neighbor_relations, user_embeddings):\n",
        "        # self_vectors: [batch_size, emb_dim]\n",
        "        # neighbor_vectors: [batch_size, n_neighbors, emb_dim]\n",
        "        neighbor_mean = neighbor_vectors.mean(dim=1)  # [batch_size, emb_dim]\n",
        "        out = torch.cat([self_vectors, neighbor_mean], dim=-1)  # [batch_size, emb_dim * 2]\n",
        "        out = self.linear(out)\n",
        "        out = self.dropout(out)\n",
        "        return self.activation(out)\n",
        "\n",
        "\n",
        "class KGPLStudent(nn.Module):\n",
        "    def __init__(self, cfg, n_user, n_entity, n_relation, adj_entity, adj_relation, path_list_dict, name, eval_mode=False):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.name = name\n",
        "        self.n_user = n_user\n",
        "        self.n_entity = n_entity\n",
        "        self.n_relation = n_relation\n",
        "        self.batch_size = cfg.optimize['batch_size']\n",
        "        self.adj_entity = adj_entity\n",
        "        self.adj_relation = adj_relation\n",
        "        self.path_list_dict = path_list_dict\n",
        "        self.eval_mode = eval_mode\n",
        "\n",
        "        self.user_emb_matrix = nn.Embedding(n_user, cfg.emb_dim)\n",
        "        self.entity_emb_matrix = nn.Embedding(n_entity, cfg.emb_dim)\n",
        "        self.relation_emb_matrix = nn.Embedding(n_relation, cfg.emb_dim)\n",
        "\n",
        "        self.aggregators = nn.ModuleList([\n",
        "            SumAggregatorWithDropout(cfg.emb_dim, cfg.dropout_rate, activation=nn.Tanh(), cfg=cfg)\n",
        "            if i == cfg.n_iter - 1 else\n",
        "            SumAggregatorWithDropout(cfg.emb_dim, cfg.dropout_rate, activation=nn.LeakyReLU(), cfg=cfg)\n",
        "            for i in range(cfg.n_iter)\n",
        "        ])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.user_emb_matrix.weight)\n",
        "        nn.init.xavier_uniform_(self.entity_emb_matrix.weight)\n",
        "        nn.init.xavier_uniform_(self.relation_emb_matrix.weight)\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embeds = self.user_emb_matrix(user_indices)\n",
        "        item_embeds = self.get_item_embeddings(item_indices)\n",
        "        scores = (user_embeds * item_embeds).sum(dim=-1)\n",
        "        return scores\n",
        "\n",
        "    def get_item_embeddings(self, item_indices):\n",
        "        entities = [item_indices.unsqueeze(1)]\n",
        "        relations = []\n",
        "        for i in range(self.cfg.n_iter):\n",
        "            neighbor_entities = self.adj_entity[entities[-1]].view(item_indices.size(0), -1)\n",
        "            neighbor_relations = self.adj_relation[entities[-1]].view(item_indices.size(0), -1)\n",
        "            entities.append(neighbor_entities)\n",
        "            relations.append(neighbor_relations)\n",
        "\n",
        "        entity_vectors = [self.entity_emb_matrix(e) for e in entities]\n",
        "        relation_vectors = [self.relation_emb_matrix(r) for r in relations]\n",
        "\n",
        "        for i in range(self.cfg.n_iter):\n",
        "            new_vectors = []\n",
        "            for hop in range(self.cfg.n_iter - i):\n",
        "                batch_size, neighbor_size, emb_dim = entity_vectors[hop+1].size(0), entity_vectors[hop+1].size(1) // self.cfg.model.neighbor_sample_size, entity_vectors[hop+1].size(2)\n",
        "                neighbor_vecs = entity_vectors[hop+1].view(batch_size, neighbor_size, self.cfg.model.neighbor_sample_size, emb_dim)\n",
        "                relation_vecs = relation_vectors[hop].view(batch_size, neighbor_size, self.cfg.model.neighbor_sample_size, emb_dim)\n",
        "                vector = self.aggregators[i](\n",
        "                    self_vectors=entity_vectors[hop],\n",
        "                    neighbor_vectors=neighbor_vecs,\n",
        "                    neighbor_relations=relation_vecs,\n",
        "                    user_embeddings=None  # optional\n",
        "                )\n",
        "                new_vectors.append(vector)\n",
        "            entity_vectors = new_vectors\n",
        "\n",
        "        return entity_vectors[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "Z6gdtS2YmFak"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        user, pos_item, neg_item, pseudo_item = batch\n",
        "        user = user.to(device)\n",
        "        pos_item = pos_item.to(device)\n",
        "        neg_item = neg_item.to(device)\n",
        "        pseudo_item = pseudo_item.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pos_scores = model(user, pos_item)\n",
        "        neg_scores = model(user, neg_item)\n",
        "        pseudo_scores = model(user, pseudo_item)\n",
        "\n",
        "        loss = kgpl_loss(pos_scores, neg_scores, pseudo_scores)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "YAvU2ClamFUw",
        "outputId": "0e597a7d-5d51-46a4-f3c9-a679bc9e0bb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-252-bbffcb7c2058>\", line 57, in __getitem__\n    pseudo_label = self.sample_pseudo_label(user)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-252-bbffcb7c2058>\", line 47, in sample_pseudo_label\n    udst, F = self.reachable_items[user]\n              ~~~~~~~~~~~~~~~~~~~~^^^^^^\nIndexError: index 9 is out of bounds for dimension 0 with size 0\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-261-f46f4d5f6233>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch} | Loss: {loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-260-4a061aef9c36>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpseudo_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-252-bbffcb7c2058>\", line 57, in __getitem__\n    pseudo_label = self.sample_pseudo_label(user)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-252-bbffcb7c2058>\", line 47, in sample_pseudo_label\n    udst, F = self.reachable_items[user]\n              ~~~~~~~~~~~~~~~~~~~~^^^^^^\nIndexError: index 9 is out of bounds for dimension 0 with size 0\n"
          ]
        }
      ],
      "source": [
        "train_dataset = data[2]\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=4,  # optional for faster loading\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = KGPLStudent(cfg, 1872, 3846, 60, train_dataset.entity_adj, train_dataset.relation_adj, train_dataset.path_list_dict, name='student').to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.optimize['lr'])\n",
        "\n",
        "# Train\n",
        "for epoch in tqdm(range(10)):\n",
        "    loss = train_one_epoch(model, train_loader, optimizer, device)\n",
        "    print(f\"Epoch {epoch} | Loss: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8waBJjGUmFOX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVPpzouPmFEX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFZ3W3qxm74R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-px9MxVm79p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek2btkFXm8BQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy7TVm4Lm8FE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo_Eur88m8JS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW8pTa-tm8Ok"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGKWBFJhToVV"
      },
      "source": [
        "# JUNK BELOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "E9bBeRhAUbBc"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "\n",
        "def grouper(n, iterable, squash=None):\n",
        "    it = iter(iterable)\n",
        "    while True:\n",
        "        if squash:\n",
        "            chunk = [\n",
        "                [None if (j != 0 and i in squash) else el[i] for i in range(len(el))]\n",
        "                for j, el in enumerate(itertools.islice(it, n))\n",
        "            ]\n",
        "        else:\n",
        "            chunk = list(itertools.islice(it, n))\n",
        "\n",
        "        if not chunk:\n",
        "            return\n",
        "        elif len(chunk) != n:\n",
        "            chunk += [None] * (n - len(chunk))\n",
        "        yield chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "cr3MmOjeTnib"
      },
      "outputs": [],
      "source": [
        "def compute_reachable_items_(args_list):\n",
        "    \"\"\"Construct the sampling distributions based on paths in KG.\n",
        "    Args:\n",
        "        args_list: list of list of arguments. Each arguments' list must contains;\n",
        "        (1) user_id;\n",
        "        (2) user's interacted item ids (seed items);\n",
        "        (3) item-to-(item, #paths) dict found in the BFS (start and end points of some paths);\n",
        "        (4) item-to-frequency dict;\n",
        "        (5) power coefficient to control the skewness of sampling distributions\n",
        "    Returns:\n",
        "        dict in which (key, value) = (item list, np.array of sampling distribution).\n",
        "        sampling distribution is transformed to CDF for fast sampling.\n",
        "    \"\"\"\n",
        "    idd = {}\n",
        "    _, _, dst_dict, item_freq, pn = args_list[0]\n",
        "    for args in args_list:\n",
        "        if args is None:\n",
        "            continue\n",
        "        user, seed_items, _, _, _ = args\n",
        "\n",
        "        # Collect user's reachable items with the number of reachable paths\n",
        "        dst = Counter()\n",
        "        for item in seed_items:\n",
        "            if item in dst_dict:\n",
        "                dst += dst_dict[item]\n",
        "\n",
        "        if len(dst) != 0:\n",
        "            # Unique reachable items for the user\n",
        "            udst = np.array(tuple(dst.keys()))\n",
        "\n",
        "            # Histogram of paths with power transform\n",
        "            F = np.array(tuple(dst.values())) ** pn\n",
        "\n",
        "            # Remove the seed (positve) items\n",
        "            inds = ~np.isin(udst, seed_items)\n",
        "            udst = udst[inds]\n",
        "            F = F[inds]\n",
        "\n",
        "            # Compute unreachable items and concat those to the end of item lists\n",
        "            udst = set(udst)\n",
        "            unreachable_items = [i for i in item_freq if i not in udst]\n",
        "            udst = list(udst) + unreachable_items\n",
        "\n",
        "            # For unreachable items, assume 0.5 virtual paths for full support\n",
        "            F = np.concatenate([F, np.ones(len(unreachable_items)) * 0.5])\n",
        "\n",
        "            # Transform histogram to CDF\n",
        "            sort_inds = np.argsort(F)\n",
        "            udst = [udst[i] for i in sort_inds]\n",
        "            F = F[sort_inds]\n",
        "            F = (F / np.sum(F)).cumsum()\n",
        "            idd[user] = (udst, F)\n",
        "    return idd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "m3wm5lTtT3L-"
      },
      "outputs": [],
      "source": [
        "def compute_user_unseen_items_(args_list):\n",
        "    user_unseen_items = {}\n",
        "    all_items, _, _ = args_list[0]\n",
        "    for args in args_list:\n",
        "        if args is None:\n",
        "            continue\n",
        "        _, user, seed_items, _, _ = args\n",
        "        unseen_items = tuple(all_items - seed_items)\n",
        "        user_unseen_items[user] = (unseen_items, None)\n",
        "    return user_unseen_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "pEispFmzkekS"
      },
      "outputs": [],
      "source": [
        "class KaPLMixin:\n",
        "    def __init__(self, cfg, n_entity, n_relation, adj_entity, adj_relation, path_list_dict, eval_mode=False):\n",
        "        self.cfg = cfg\n",
        "        self.n_entity = n_entity\n",
        "        self.n_relation = n_relation\n",
        "        self.user_seed_dict = defaultdict(set)\n",
        "        self.item_dist_dict = {}\n",
        "        self.cand_uinds = None\n",
        "        self.cand_iinds = None\n",
        "\n",
        "    def _build_freq_dict(self, seq, all_candidates):\n",
        "        _freq = Counter(seq)\n",
        "        for i in all_candidates:\n",
        "            if i not in _freq:\n",
        "                _freq[i] += 1\n",
        "        freq = [_freq[i] for i in all_candidates]\n",
        "        return dict(zip(all_candidates, freq))\n",
        "\n",
        "    def set_item_candidates(self, n_user, n_item, train_data, eval_data, path_list_dict):\n",
        "        all_users = tuple(set(train_data[:, 0]))\n",
        "        self.all_users = all_users\n",
        "        self.n_item = n_item\n",
        "        self.all_items = set(range(n_item))\n",
        "\n",
        "        self.neg_c_dict_user = self._build_freq_dict(\n",
        "            np.concatenate([train_data[:, 0], eval_data[:, 0]]), self.all_users\n",
        "        )\n",
        "        self.neg_c_dict_item = self._build_freq_dict(\n",
        "            np.concatenate([train_data[:, 1], eval_data[:, 1]]), self.all_items\n",
        "        )\n",
        "\n",
        "        item_cands = tuple(self.neg_c_dict_item.keys())\n",
        "        F = np.array(list(self.neg_c_dict_item.values())) ** self.cfg.plabel['neg_pn']\n",
        "        sort_inds = np.argsort(F)\n",
        "        item_cands = [item_cands[i] for i in sort_inds]\n",
        "        F = F[sort_inds]\n",
        "        F = F / F.sum()\n",
        "        F = np.cumsum(F)\n",
        "        self.item_freq = (item_cands, F)\n",
        "\n",
        "        for u, i in train_data[:, 0:2]:\n",
        "            self.user_seed_dict[u].add(i)\n",
        "\n",
        "        self._setup_dst_dict(path_list_dict)\n",
        "        item_dist_dict = {}\n",
        "\n",
        "        src_itr = [\n",
        "            (all_users[iu], tuple(self.user_seed_dict[all_users[iu]]), self.dst_dict,\n",
        "             self.neg_c_dict_item, self.cfg.plabel.pl_pn)\n",
        "            for iu in range(len(all_users))\n",
        "        ]\n",
        "\n",
        "        # Using multiprocess (if you want parallelism) - here shown sequentially\n",
        "        for idd in map(compute_reachable_items_, [src_itr[i:i+self.cfg.plabel.chunk_size] for i in range(0, len(src_itr), self.cfg.plabel.chunk_size)]):\n",
        "            item_dist_dict.update(idd)\n",
        "\n",
        "        self.item_dist_dict = item_dist_dict\n",
        "\n",
        "    def _setup_dst_dict(self, path_list_dict):\n",
        "        dst_dict = {}\n",
        "        for item, paths in path_list_dict.items():\n",
        "            dst = []\n",
        "            for p in paths:\n",
        "                dst.append(p[-1])\n",
        "            dst_dict[item] = Counter(dst)\n",
        "        self.dst_dict = dst_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "q47kMAMaksCs"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class KGPLStudent(nn.Module, KaPLMixin):\n",
        "    def __init__(self, cfg, n_user, n_entity, n_relation, adj_entity, adj_relation, path_list_dict, name, eval_mode=False):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.cfg = cfg\n",
        "        self.n_user = n_user\n",
        "        self.n_entity = n_entity\n",
        "        self.n_relation = n_relation\n",
        "        self.batch_size = cfg.optimize.batch_size\n",
        "\n",
        "        # Embedding matrices\n",
        "        self.user_emb_matrix = nn.Embedding(n_user, cfg.emb_dim)\n",
        "        self.entity_emb_matrix = nn.Embedding(n_entity, cfg.emb_dim)\n",
        "        self.relation_emb_matrix = nn.Embedding(n_relation, cfg.emb_dim)\n",
        "\n",
        "        # Neighborhood info\n",
        "        self.adj_entity = torch.tensor(adj_entity, dtype=torch.long)\n",
        "        self.adj_relation = torch.tensor(adj_relation, dtype=torch.long)\n",
        "\n",
        "        KaPLMixin.__init__(self, cfg, n_entity, n_relation, adj_entity, adj_relation, path_list_dict, eval_mode)\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embeddings = self.user_emb_matrix(user_indices)\n",
        "        entities, relations = self.get_neighbors(item_indices)\n",
        "        item_embeddings = self.aggregate(entities, relations)\n",
        "        scores = (user_embeddings * item_embeddings).sum(dim=1)\n",
        "        return torch.sigmoid(scores)\n",
        "\n",
        "    def get_neighbors(self, seeds):\n",
        "        entities = [seeds.unsqueeze(1)]\n",
        "        relations = []\n",
        "        for _ in range(self.cfg.n_iter):\n",
        "            neighbor_entities = self.adj_entity[entities[-1]].view(seeds.size(0), -1)\n",
        "            neighbor_relations = self.adj_relation[entities[-1]].view(seeds.size(0), -1)\n",
        "            entities.append(neighbor_entities)\n",
        "            relations.append(neighbor_relations)\n",
        "        return entities, relations\n",
        "\n",
        "    def aggregate(self, entities, relations):\n",
        "        entity_vectors = [self.entity_emb_matrix(entity) for entity in entities]\n",
        "        relation_vectors = [self.relation_emb_matrix(rel) for rel in relations]\n",
        "        for i in range(self.cfg.n_iter):\n",
        "            if i == self.cfg.n_iter - 1:\n",
        "                act = torch.tanh\n",
        "            else:\n",
        "                act = F.leaky_relu\n",
        "            entity_vectors_next_iter = []\n",
        "            for hop in range(self.cfg.n_iter - i):\n",
        "                self_vector = entity_vectors[hop]\n",
        "                neighbor_vector = entity_vectors[hop + 1].view(self_vector.size(0), -1, self.cfg.neighbor_sample_size, self.cfg.emb_dim)\n",
        "                neighbor_relation = relation_vectors[hop].view(self_vector.size(0), -1, self.cfg.neighbor_sample_size, self.cfg.emb_dim)\n",
        "                # Simple sum aggregation\n",
        "                vector = act(self_vector + neighbor_vector.mean(2) + neighbor_relation.mean(2))\n",
        "                entity_vectors_next_iter.append(vector)\n",
        "            entity_vectors = entity_vectors_next_iter\n",
        "        res = entity_vectors[0].view(self.batch_size, self.cfg.emb_dim)\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50eFL91dkr4c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH3dugxJkeTG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bEyIl3vUNIL"
      },
      "outputs": [],
      "source": [
        "class KaPLMixin(object):\n",
        "    def __init__(\n",
        "        self, cfg, n_entity, n_relation, adj_entity, adj_relation, path_list_dict, eval_mode=False\n",
        "    ):\n",
        "        self.user_seed_dict = defaultdict(set)\n",
        "        self.item_dist_dict = {}\n",
        "        self.cand_uinds = None\n",
        "        self.cand_iinds = None\n",
        "        self.n_entity = n_entity\n",
        "        self.n_relation = n_relation\n",
        "\n",
        "    def _build_freq_dict(self, seq, all_candidates):\n",
        "        _freq = Counter(seq)\n",
        "        for i in all_candidates:\n",
        "            if i not in _freq:\n",
        "                _freq[i] += 1\n",
        "        freq = [_freq[i] for i in all_candidates]\n",
        "        return dict(zip(all_candidates, freq))\n",
        "\n",
        "    def set_item_candidates(\n",
        "        self, n_user, n_item, train_data, eval_data, path_list_dict\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Construct the sampling distrbiutions for negative/pseudo-labelled instances for each user\n",
        "        \"\"\"\n",
        "        all_users = tuple(set(train_data[:, 0]))\n",
        "        self.all_users = all_users\n",
        "\n",
        "        self.n_item = n_item\n",
        "        self.all_items = set(range(n_item))\n",
        "        self.neg_c_dict_user = self._build_freq_dict(\n",
        "            np.concatenate([train_data[:, 0], eval_data[:, 0]]), self.all_users\n",
        "        )\n",
        "        self.neg_c_dict_item = self._build_freq_dict(\n",
        "            np.concatenate([train_data[:, 1], eval_data[:, 1]]), self.all_items\n",
        "        )\n",
        "\n",
        "        item_cands = tuple(self.neg_c_dict_item.keys())\n",
        "        F = np.array(tuple(self.neg_c_dict_item.values())) ** self.cfg.plabel.neg_pn\n",
        "        sort_inds = np.argsort(F)\n",
        "        item_cands = [item_cands[i] for i in sort_inds]\n",
        "        F = F[sort_inds]\n",
        "        F = (F / F.sum()).cumsum()\n",
        "        self.item_freq = (item_cands, F)\n",
        "\n",
        "        for u, i in tqdm(train_data[:, 0:2]):\n",
        "            self.user_seed_dict[u].add(i)\n",
        "\n",
        "        # path = hydra.utils.to_absolute_path(self.cfg.reachable_items_path)\n",
        "        print(\"calculating reachable items for users\")\n",
        "        self._setup_dst_dict(path_list_dict)\n",
        "        item_dist_dict = {}\n",
        "        src_itr = map(\n",
        "            lambda iu: (\n",
        "                all_users[iu],\n",
        "                tuple(self.user_seed_dict[all_users[iu]]),\n",
        "                self.dst_dict,\n",
        "                self.neg_c_dict_item,\n",
        "                self.cfg.plabel.pl_pn,\n",
        "            ),\n",
        "            range(len(all_users)),\n",
        "        )\n",
        "        grouped = grouper(self.cfg.plabel.chunk_size, src_itr, squash=set([2, 3]))\n",
        "        with mp.Pool(self.cfg.plabel.par) as pool:\n",
        "            for idd in pool.imap_unordered(compute_reachable_items_, grouped):\n",
        "                item_dist_dict.update(idd)\n",
        "        self.item_dist_dict = item_dist_dict\n",
        "\n",
        "    def _setup_dst_dict(self, path_list_dict):\n",
        "        \"\"\"\n",
        "        Transform path representations:\n",
        "        `list of nodes` to `dictionaly of source to sink (dst_dict)`\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"setup dst dict...\")\n",
        "        dst_dict = {}\n",
        "        for item in tqdm(path_list_dict):\n",
        "            dst = []\n",
        "            paths = path_list_dict[item]\n",
        "            for i, p in enumerate(paths):\n",
        "                dst.append(p[-1])\n",
        "            dst_dict[item] = Counter(dst)\n",
        "        print(\"start updating path info...\")\n",
        "        self.dst_dict = dst_dict\n",
        "        print.info(\"path info updated.\")\n",
        "\n",
        "    # def _get_user_rel_scores(self, sess, users):\n",
        "    def _get_user_rel_scores(self, users)\n",
        "        uembs = self.user_embeddings()\n",
        "\n",
        "        # self.user_indices = users\n",
        "\n",
        "        uembs =\n",
        "\n",
        "        # sess.run(\n",
        "        #     self.user_embeddings, feed_dict={self.user_indices: users, self.dropout_rate: 0.0}\n",
        "        # )  # nu, legth\n",
        "        # rembs = sess.run(self.relation_emb_matrix)  # nr, length\n",
        "        rembs = self.relation_emb_matrix()\n",
        "\n",
        "        return np.dot(uembs, rembs.T)  # nu, nr\n",
        "\n",
        "    def _get_mini_batch_pl(self, sess, users):\n",
        "        \"\"\"\n",
        "        Create pseudo-labelled instances for users\n",
        "        \"\"\"\n",
        "        pl_users, pl_items = [], []\n",
        "        ind = 0\n",
        "        cands, freq_F = self.item_freq\n",
        "        while True:\n",
        "            u = users[ind % len(users)]\n",
        "            ind += 1\n",
        "            if u in self.item_dist_dict and len(self.item_dist_dict[u][0]) != 0:\n",
        "                udst, F = self.item_dist_dict[u]\n",
        "                i = udst[np.searchsorted(F, torch.rand(1).item())] #changed random to torch\n",
        "            else:\n",
        "                while True:\n",
        "                    i = cands[np.searchsorted(freq_F, torch.rand(1).item())] #changed random to torch\n",
        "                    if i not in self.user_seed_dict[u]:\n",
        "                        break\n",
        "            pl_users.append(u)\n",
        "            pl_items.append(i)\n",
        "            if len(pl_users) == len(users):\n",
        "                break\n",
        "\n",
        "        pl_users_pad = list(pl_users) + [0] * (self.batch_size - len(pl_users))\n",
        "        pl_items_pad = list(pl_items) + [0] * (self.batch_size - len(pl_items))\n",
        "\n",
        "        # start taylor added for PyTorch\n",
        "        # self.user_indices = pl_users_pad\n",
        "        # self.item_indices = pl_items_pad\n",
        "        # self.scores_normalized = self._build_model(n_user, n_entity, n_relation)\n",
        "        # end taylor addded pytorch\n",
        "\n",
        "        pl_labels_pad = sess.run(\n",
        "            self.scores_normalized,\n",
        "            feed_dict={\n",
        "                self.user_indices: pl_users_pad,\n",
        "                self.item_indices: pl_items_pad,\n",
        "                self.dropout_rate: 0.0,\n",
        "            },\n",
        "        )\n",
        "        pl_users = pl_users_pad[: len(pl_users)]\n",
        "        pl_items = pl_items_pad[: len(pl_items)]\n",
        "        pl_labels = pl_labels_pad[: len(pl_users)]\n",
        "        return pl_users, pl_items, pl_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLyzfKD1tjnp"
      },
      "source": [
        "## Train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "H6Vu93_7NoT6"
      },
      "outputs": [],
      "source": [
        "class KGPL_COT():\n",
        "  def __init__(self, *args):\n",
        "    pass\n",
        "\n",
        "def topk_settings(*args, **kwargs):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LNn4UQ9auEv",
        "outputId": "fc5d73da-6023-4ff4-c3f5-c4b1b69ea6e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num train records: 27102\n",
            "num adj entities: 9366, num entities: 9366\n",
            "num adj relations: 9366, num relations: 60\n",
            "model type: KGPL_COT\n",
            "<__main__.KGPL_COT object at 0x7d6c76b49dd0>\n"
          ]
        }
      ],
      "source": [
        "def train(cfg, data):\n",
        "    (n_user, n_item, train_data, eval_data, test_data) = data\n",
        "\n",
        "    adj_entity = train_data.entity_adj\n",
        "    adj_relation = train_data.relation_adj\n",
        "    n_entity = adj_entity.shape[0]\n",
        "    n_relation = len(torch.unique(adj_relation.reshape(-1)))\n",
        "    path_list_dict = train_data.path_list_dict\n",
        "\n",
        "    print(f\"num train records: {len(train_data)}\")\n",
        "    print(f\"num adj entities: {len(adj_entity)}, num entities: {n_entity}\")\n",
        "    print(f\"num adj relations: {len(adj_relation)}, num relations: {n_relation}\")\n",
        "\n",
        "    model = KGPL_COT(\n",
        "        cfg,\n",
        "        n_user,\n",
        "        n_item,\n",
        "        n_entity,\n",
        "        n_relation,\n",
        "        adj_entity,\n",
        "        adj_relation,\n",
        "        path_list_dict,\n",
        "        train_data,\n",
        "        eval_data,\n",
        "    )\n",
        "\n",
        "    _pos_inds = train_data[:, 2] == 1\n",
        "    train_data = train_data[_pos_inds]\n",
        "    print(\"model type:\", cfg.model_type)\n",
        "\n",
        "    topk_config = topk_settings(\n",
        "        train_data,\n",
        "        eval_data,\n",
        "        test_data,\n",
        "        n_item,\n",
        "        test_mode=True,\n",
        "        user_num=cfg.evaluate['user_num_topk'],\n",
        "    )\n",
        "\n",
        "    batch_size = cfg.optimize['batch_size']\n",
        "\n",
        "    print(model)\n",
        "\n",
        "train(cfg, data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
