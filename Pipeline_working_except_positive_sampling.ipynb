{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# %pip install hydra-core==0.11.3\n",
        "# %pip install omegaconf==1.4.1\n",
        "# %pip install loguru==0.5.0"
      ],
      "metadata": {
        "id": "e9-KKdaxCHYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5nBoNooNB6kU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import multiprocessing as mp\n",
        "import itertools\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/riktor/KGPL/"
      ],
      "metadata": {
        "id": "-3q187KhB8up"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd KGPL &&\\\n",
        "# python preprocess/preprocess.py -d \"music\" &&\\\n",
        "# python preprocess/make_path_list.py lp_depth=6 dataset=music kg_path=data/music/kg_final.npy rating_path=data/music/ratings_final.npy num_neighbor_samples=32"
      ],
      "metadata": {
        "id": "6TJnHlXfB-Iu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "bmQ4fOo3bQrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_dst_dict(path_list_dict):\n",
        "        \"\"\"\n",
        "        Transform path representations:\n",
        "        `list of nodes` to `dictionary of source to sink (dst_dict)`\n",
        "        \"\"\"\n",
        "        print(\"Setting up dst dict...\")\n",
        "        dst_dict = {}\n",
        "        for item in tqdm(path_list_dict):\n",
        "            dst = []\n",
        "            paths = path_list_dict[item]\n",
        "            for p in paths:\n",
        "                dst.append(p[-1])\n",
        "            dst_dict[item] = Counter(dst)\n",
        "\n",
        "        print(\"Start updating path info...\")\n",
        "        print(\"Path info updated.\")\n",
        "        return dst_dict\n",
        "\n",
        "def grouper(n, iterable, squash=None):\n",
        "    it = iter(iterable)\n",
        "    while True:\n",
        "        if squash:\n",
        "            chunk = [\n",
        "                [None if (j != 0 and i in squash) else el[i] for i in range(len(el))]\n",
        "                for j, el in enumerate(itertools.islice(it, n))\n",
        "            ]\n",
        "        else:\n",
        "            chunk = list(itertools.islice(it, n))\n",
        "\n",
        "        if not chunk:\n",
        "            return\n",
        "        elif len(chunk) != n:\n",
        "            chunk += [None] * (n - len(chunk))\n",
        "        yield chunk\n",
        "\n",
        "def compute_reachable_items_(args_list):\n",
        "  \"\"\"Construct the sampling distributions based on paths in KG.\n",
        "  Args:\n",
        "      args_list: list of list of arguments. Each arguments' list must contains;\n",
        "      (1) user_id;\n",
        "      (2) user's interacted item ids (seed items);\n",
        "      (3) item-to-(item, #paths) dict found in the BFS (start and end points of some paths);\n",
        "      (4) item-to-frequency dict;\n",
        "      (5) power coefficient to control the skewness of sampling distributions\n",
        "  Returns:\n",
        "      dict in which (key, value) = (item list, np.array of sampling distribution).\n",
        "      sampling distribution is transformed to CDF for fast sampling.\n",
        "  \"\"\"\n",
        "  idd = {}\n",
        "  _, _, dst_dict, item_freq, pn = args_list[0]\n",
        "  for args in args_list:\n",
        "      if args is None:\n",
        "          continue\n",
        "      user, seed_items, _, _, _ = args\n",
        "\n",
        "      # print('User:', user)\n",
        "      # print('Seed Items:', seed_items)\n",
        "\n",
        "      # Collect user's reachable items with the number of reachable paths\n",
        "      dst = Counter()\n",
        "      for item in seed_items:\n",
        "          if item in dst_dict:\n",
        "              dst += dst_dict[item]\n",
        "\n",
        "      if len(dst) != 0:\n",
        "          # Unique reachable items for the user\n",
        "          udst = np.array(tuple(dst.keys()))\n",
        "\n",
        "          # Histogram of paths with power transform\n",
        "          F = np.array(tuple(dst.values())) ** pn\n",
        "\n",
        "          # Remove the seed (positve) items\n",
        "          inds = ~np.isin(udst, seed_items)\n",
        "          udst = udst[inds]\n",
        "          F = F[inds]\n",
        "\n",
        "          # Compute unreachable items and concat those to the end of item lists\n",
        "          udst = set(udst)\n",
        "          unreachable_items = [i for i in item_freq if i not in udst]\n",
        "          udst = list(udst) + unreachable_items\n",
        "\n",
        "          # For unreachable items, assume 0.5 virtual paths for full support\n",
        "          F = np.concatenate([F, np.ones(len(unreachable_items)) * 0.5])\n",
        "\n",
        "          # Transform histogram to CDF\n",
        "          sort_inds = np.argsort(F)\n",
        "          udst = [udst[i] for i in sort_inds]\n",
        "          F = F[sort_inds]\n",
        "          F = (F / np.sum(F)).cumsum()\n",
        "          idd[user] = (udst, F)\n",
        "  return idd\n",
        "\n",
        "# HELPER FUNCTION STRAIGHT FROM CHATGPT\n",
        "def build_user_train_dict_from_tensor(ratings):\n",
        "    user_train = defaultdict(list)\n",
        "    for user, item, rating in ratings:\n",
        "        if rating >= 1:  # or whatever you define as \"positive\" interaction\n",
        "            user = user.item()\n",
        "            item = item.item()\n",
        "            # if user not in user_train:\n",
        "            #     user_train[user] = []\n",
        "            user_train[user].append(item)\n",
        "    return user_train"
      ],
      "metadata": {
        "id": "Wp6sO1ulCAyt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Dataset Class"
      ],
      "metadata": {
        "id": "QbSzL0gWdyhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KGPL_Dataset(Dataset):\n",
        "  '''\n",
        "  Custom dataset class which includes all datasets and parameters per model.\n",
        "  Specified under \"data\" directory\n",
        "  '''\n",
        "\n",
        "  base_data_path = 'KGPL/data/'\n",
        "  datasets = {}\n",
        "  n_user = 0\n",
        "  n_item = 0\n",
        "\n",
        "  class cfg:\n",
        "    plabel_lp_depth = 6\n",
        "    plabel_par = 16\n",
        "    plabel_chunk_size = 250\n",
        "    plabel_neg_pn = 0.1\n",
        "    plabel_pl_pn = 1e-3\n",
        "\n",
        "  @classmethod\n",
        "  def _init_class(cls, dataset_name):\n",
        "    cls.dataset_name = dataset_name\n",
        "    print('Loading Entity Adjacencies...')\n",
        "    cls.adj_entity = torch.from_numpy(np.load(cls.base_data_path + dataset_name + '/adj_entity_6_32.npy'))\n",
        "    print('Loading Relation Adjacencies...')\n",
        "    cls.adj_relation = torch.from_numpy(np.load(cls.base_data_path + dataset_name + '/adj_relation_6_32.npy'))\n",
        "    print('Loading Ratings...')\n",
        "    cls.ratings = torch.from_numpy(np.load(cls.base_data_path + dataset_name + '/ratings_final.npy'))\n",
        "    print('Loading Path List...')\n",
        "    cls.path_list_dict = pickle.load(open(cls.base_data_path + dataset_name + '/path_list_6_32.pkl', 'rb'))\n",
        "    print('Loading Distances...')\n",
        "    cls.dst_dict = setup_dst_dict(cls.path_list_dict)\n",
        "\n",
        "  def _split_data(self, split_ratio=0.2):\n",
        "    #split dataset\n",
        "    n_ratings = len(self.ratings)\n",
        "    split_indices = torch.randperm(n_ratings)[:int(n_ratings * split_ratio)]\n",
        "    splitted_data = self.ratings[split_indices]\n",
        "    rest_data = self.ratings[~torch.isin(torch.arange(n_ratings), split_indices)]\n",
        "    #create new objects\n",
        "    splitted_dataset, rest_dataset = deepcopy(self), deepcopy(self)\n",
        "    splitted_dataset.ratings = splitted_data\n",
        "    rest_dataset.ratings = rest_data\n",
        "    return rest_dataset, splitted_dataset\n",
        "\n",
        "  def _readjust_counts(self):\n",
        "    self.users = torch.unique(self.ratings[:,0])\n",
        "    self.items = torch.unique(self.ratings[:,1])\n",
        "\n",
        "  def _train_val_test_split(self):\n",
        "    exp_dataset, test = self._split_data()\n",
        "    train, val = exp_dataset._split_data()\n",
        "    train.train_set = True\n",
        "    val.train_set = False\n",
        "    test.train_set = False\n",
        "    for fold in (train, val, test):\n",
        "      fold._readjust_counts()\n",
        "    n_user = torch.unique(self.ratings[:,0]).numel()\n",
        "    n_item = torch.unique(self.ratings[:,1]).numel()\n",
        "    train._readjust_counts()\n",
        "    val._readjust_counts()\n",
        "    test._readjust_counts()\n",
        "    self.__class__.n_user = n_user\n",
        "    self.__class__.n_item = n_item\n",
        "    self.__class__.datasets['train'] = train\n",
        "    self.__class__.datasets['val'] = val\n",
        "    self.__class__.datasets['test'] = test\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def _build_freq_dict(seq, all_candidates): # need all_candidates\n",
        "    _freq = Counter(seq)\n",
        "    for i in all_candidates:\n",
        "        if i not in _freq:\n",
        "            _freq[i] += 1\n",
        "    freq = [_freq[i] for i in all_candidates]\n",
        "    return dict(zip(all_candidates, freq))\n",
        "\n",
        "  #### THIS IS THE BIG ONE #####\n",
        "  def set_item_candidates(self):\n",
        "        \"\"\"\n",
        "        Construct the sampling distrbiutions for negative/pseudo-labelled instances for each user\n",
        "        \"\"\"\n",
        "        train_data = self.datasets['train'].ratings\n",
        "        eval_data = self.datasets['val'].ratings\n",
        "        # all_users = tuple(set(train_data[:, 0])) # train data sets the users - not predicting unseen users\n",
        "        all_users = torch.unique(train_data[:,0])\n",
        "        self.all_users = all_users\n",
        "        self.all_items = set(torch.arange(self.__class__.n_item))\n",
        "        self.neg_c_dict_user = self._build_freq_dict(\n",
        "            torch.concat([train_data[:, 0], eval_data[:, 0]]), self.all_users\n",
        "        )\n",
        "\n",
        "        print('Neg C Dict User:', len(self.neg_c_dict_user))\n",
        "\n",
        "        self.neg_c_dict_item = self._build_freq_dict(\n",
        "            np.concatenate([train_data[:, 1], eval_data[:, 1]]), self.all_items\n",
        "        )\n",
        "\n",
        "        print('Neg C Dict Item:', len(self.neg_c_dict_item))\n",
        "\n",
        "        item_cands = tuple(self.neg_c_dict_item.keys())\n",
        "        F = np.array(tuple(self.neg_c_dict_item.values())) ** self.cfg.plabel_neg_pn\n",
        "        sort_inds = np.argsort(F)\n",
        "        item_cands = [item_cands[i] for i in sort_inds]\n",
        "        F = F[sort_inds]\n",
        "        F = (F / F.sum()).cumsum()\n",
        "        self.item_freq = (item_cands, F)\n",
        "\n",
        "        for u, i in tqdm(train_data[:, 0:2]):\n",
        "            self.user_seed_dict[u.item()].add(i.item())\n",
        "\n",
        "        # path = hydra.utils.to_absolute_path(self.cfg.reachable_items_path)\n",
        "        # logger.info(\"calculating reachable items for users\")\n",
        "        # self._setup_dst_dict(self.path_list_dict)\n",
        "        item_dist_dict = {}\n",
        "        src_itr = map(\n",
        "            lambda iu: (\n",
        "                all_users[iu].item(),\n",
        "                tuple(self.user_seed_dict[all_users[iu].item()]),\n",
        "                self.dst_dict,\n",
        "                self.neg_c_dict_item,\n",
        "                self.cfg.plabel_pl_pn,\n",
        "            ),\n",
        "            range(len(all_users)),\n",
        "        )\n",
        "\n",
        "        # print('SRC ITR:', len(src_itr))\n",
        "\n",
        "        grouped = grouper(self.cfg.plabel_chunk_size, src_itr, squash=set([2, 3]))\n",
        "\n",
        "        # with mp.Pool(self.cfg.plabel_par) as pool:\n",
        "        #     for idd in pool.imap_unordered(compute_reachable_items_, grouped):\n",
        "        #         item_dist_dict.update(idd)\n",
        "        #         print(idd)\n",
        "        # self.item_dist_dict = item_dist_dict\n",
        "        print('Populating item dist dict...')\n",
        "        item_dist_dict = {}\n",
        "        for group in tqdm(grouped):\n",
        "            # print('Group sample:', group[0])\n",
        "            idd = compute_reachable_items_(group)\n",
        "            item_dist_dict.update(idd)\n",
        "        self.item_dist_dict = item_dist_dict\n",
        "\n",
        "  def __init__(self, dataset_name):\n",
        "    self._init_class(dataset_name)\n",
        "    # self_ratings = self.all_ratings\n",
        "    self.user_seed_dict = defaultdict(set)\n",
        "    self._train_val_test_split()\n",
        "    self.__class__.datasets['train'].set_item_candidates()\n",
        "    self.__class__.datasets['train'].user_train = build_user_train_dict_from_tensor(self.__class__.datasets['train'].ratings)\n",
        "\n",
        "  def sample_positive(self, user):\n",
        "        choice_seq = self.ratings[(self.ratings[:,0] == user) & (self.ratings[:,2]) >= 1][:,1]\n",
        "        if choice_seq.numel() == 0:\n",
        "          print('Something went wrong.')\n",
        "        return random.choice(choice_seq)\n",
        "\n",
        "  def sample_negative(self, user):\n",
        "      seen = set(self.user_train[user])\n",
        "      while True:\n",
        "          item = random.randint(0, self.n_item - 1)\n",
        "          if item not in seen:\n",
        "              return item\n",
        "\n",
        "  def sample_pseudo_label(self, user):\n",
        "        udst, F = self.item_dist_dict[user.item()]\n",
        "        r = random.random()\n",
        "        F = torch.as_tensor(F)\n",
        "        idx = torch.searchsorted(F, r, right=True)\n",
        "        return udst[idx]\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    if self.train_set:\n",
        "      user = self.users[idx-1]\n",
        "      pos_item = self.sample_positive(user)\n",
        "      neg_item = self.sample_negative(user)\n",
        "      pseudo_label = self.sample_pseudo_label(user)\n",
        "      return user, torch.tensor(pos_item), torch.tensor(neg_item), torch.tensor(pseudo_label)\n",
        "    else:\n",
        "      # still need to implement\n",
        "      return None\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_user\n",
        "\n",
        "KGPL_Dataset('music')\n",
        "\n"
      ],
      "metadata": {
        "id": "j65m9YApYsr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce233c9-f185-4a5d-de1d-530f4f273442"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Entity Adjacencies...\n",
            "Loading Relation Adjacencies...\n",
            "Loading Ratings...\n",
            "Loading Path List...\n",
            "Loading Distances...\n",
            "Setting up dst dict...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3846/3846 [00:00<00:00, 13403.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start updating path info...\n",
            "Path info updated.\n",
            "Neg C Dict User: 1872\n",
            "Neg C Dict Item: 3846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27102/27102 [00:00<00:00, 185332.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating item dist dict...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8it [00:04,  1.93it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.KGPL_Dataset at 0x787f18716a50>"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing getitem\n",
        "KGPL_Dataset.datasets['train'][55]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAxn2JAGIvd9",
        "outputId": "fe36f5b3-953a-441b-8ca6-602a2a105884"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-126-d28979224a6c>:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return user, torch.tensor(pos_item), torch.tensor(neg_item), torch.tensor(pseudo_label)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(54), tensor(302), tensor(1150), tensor(1741))"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "vVf41zoV2nXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KGPL_Config():\n",
        "  '''\n",
        "  KGPL model configuration for each dataset.\n",
        "  '''\n",
        "  def __init__(self, dataset_name:str, model_type:str, neighbor_sample_size:int, dropout_rate:float, emb_dim:int, n_iter:int, plabel:dict, optimize:dict, log:dict, evaluate:dict, model:dict):\n",
        "    self.dataset_name = dataset_name\n",
        "    self.model_type = model_type\n",
        "    self.neighbor_sample_size = neighbor_sample_size\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.emb_dim = emb_dim\n",
        "    self.n_iter = n_iter\n",
        "    self.plabel = plabel\n",
        "    self.optimize = optimize\n",
        "    self.log = log\n",
        "    self.evaluate = evaluate\n",
        "    self.model = model"
      ],
      "metadata": {
        "id": "Rn6q_sSB2mtS"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cfg = KGPL_Config(\n",
        "    'music',\n",
        "    'KGPL_COT',\n",
        "    neighbor_sample_size=32,\n",
        "    dropout_rate=0.5,\n",
        "    emb_dim=64,\n",
        "    n_iter=1,\n",
        "    plabel={},\n",
        "    optimize={'iter_per_epoch':100, 'lr': 3e-3, 'batch_size':3333},\n",
        "    log={'show_loss':True},\n",
        "    evaluate={'user_num_topk':1000},\n",
        "    model={'n_iter':1, 'neighbor_sample_size':32, 'dropout_rate':0.5}\n",
        ")"
      ],
      "metadata": {
        "id": "TrkxkKtMZMDl"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kgpl_loss(pos_scores, neg_scores, pseudo_scores):\n",
        "    # BCE loss like TensorFlow version\n",
        "    pos_labels = torch.ones_like(pos_scores)\n",
        "    neg_labels = torch.zeros_like(neg_scores)\n",
        "    pseudo_labels = torch.ones_like(pseudo_scores)\n",
        "    loss = F.binary_cross_entropy_with_logits(pos_scores, pos_labels) + \\\n",
        "           F.binary_cross_entropy_with_logits(neg_scores, neg_labels) + \\\n",
        "           F.binary_cross_entropy_with_logits(pseudo_scores, pseudo_labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "erpb_BB2QFMT"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SumAggregatorWithDropout(nn.Module):\n",
        "    def __init__(self, emb_dim, dropout_rate, activation, cfg):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(emb_dim * 2, emb_dim)\n",
        "        self.activation = activation\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, self_vectors, neighbor_vectors, neighbor_relations, user_embeddings):\n",
        "        # self_vectors: [batch_size, emb_dim]\n",
        "        # neighbor_vectors: [batch_size, n_neighbors, emb_dim]\n",
        "        # neighbor_mean = neighbor_vectors.mean(dim=1)  # [batch_size, emb_dim]\n",
        "        neighbor_mean = neighbor_vectors.mean(dim=[1, 2])\n",
        "        out = torch.cat([self_vectors, neighbor_mean], dim=-1)  # [batch_size, emb_dim * 2]\n",
        "        out = self.linear(out)\n",
        "        out = self.dropout(out)\n",
        "        return self.activation(out)\n",
        "\n",
        "\n",
        "class KGPLStudent(nn.Module):\n",
        "    def __init__(self, cfg, n_user, n_entity, n_relation, adj_entity, adj_relation, path_list_dict, name, eval_mode=False):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.name = name\n",
        "        self.n_user = n_user\n",
        "        self.n_entity = n_entity\n",
        "        self.n_relation = n_relation\n",
        "        self.batch_size = cfg.optimize['batch_size']\n",
        "        self.adj_entity = adj_entity\n",
        "        self.adj_relation = adj_relation\n",
        "        self.path_list_dict = path_list_dict\n",
        "        self.eval_mode = eval_mode\n",
        "\n",
        "        self.user_emb_matrix = nn.Embedding(n_user, cfg.emb_dim)\n",
        "        self.entity_emb_matrix = nn.Embedding(n_entity, cfg.emb_dim)\n",
        "        self.relation_emb_matrix = nn.Embedding(n_relation, cfg.emb_dim)\n",
        "\n",
        "        self.aggregators = nn.ModuleList([\n",
        "            SumAggregatorWithDropout(cfg.emb_dim, cfg.dropout_rate, activation=nn.Tanh(), cfg=cfg)\n",
        "            if i == cfg.n_iter - 1 else\n",
        "            SumAggregatorWithDropout(cfg.emb_dim, cfg.dropout_rate, activation=nn.LeakyReLU(), cfg=cfg)\n",
        "            for i in range(cfg.n_iter)\n",
        "        ])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.user_emb_matrix.weight)\n",
        "        nn.init.xavier_uniform_(self.entity_emb_matrix.weight)\n",
        "        nn.init.xavier_uniform_(self.relation_emb_matrix.weight)\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embeds = self.user_emb_matrix(user_indices)\n",
        "        item_embeds = self.get_item_embeddings(item_indices)\n",
        "        scores = (user_embeds * item_embeds).sum(dim=-1)\n",
        "        return scores\n",
        "\n",
        "    def get_item_embeddings(self, item_indices):\n",
        "        entities = [item_indices]\n",
        "        relations = []\n",
        "        for i in range(self.cfg.n_iter):\n",
        "            neighbor_entities = self.adj_entity[entities[-1]].view(item_indices.size(0), -1)\n",
        "            neighbor_relations = self.adj_relation[entities[-1]].view(item_indices.size(0), -1)\n",
        "            entities.append(neighbor_entities)\n",
        "            relations.append(neighbor_relations)\n",
        "\n",
        "        entity_vectors = [self.entity_emb_matrix(e) for e in entities]\n",
        "        relation_vectors = [self.relation_emb_matrix(r) for r in relations]\n",
        "\n",
        "        for i in range(self.cfg.n_iter):\n",
        "            new_vectors = []\n",
        "            for hop in range(self.cfg.n_iter - i):\n",
        "                batch_size, neighbor_size, emb_dim = entity_vectors[hop+1].size(0), entity_vectors[hop+1].size(1) // self.cfg.model['neighbor_sample_size'], entity_vectors[hop+1].size(2)\n",
        "                neighbor_vecs = entity_vectors[hop+1].view(batch_size, neighbor_size, self.cfg.model['neighbor_sample_size'], emb_dim)\n",
        "                relation_vecs = relation_vectors[hop].view(batch_size, neighbor_size, self.cfg.model['neighbor_sample_size'], emb_dim)\n",
        "                vector = self.aggregators[i](\n",
        "                    self_vectors=entity_vectors[hop],\n",
        "                    neighbor_vectors=neighbor_vecs,\n",
        "                    neighbor_relations=relation_vecs,\n",
        "                    user_embeddings=None  # optional\n",
        "                )\n",
        "                new_vectors.append(vector)\n",
        "            entity_vectors = new_vectors\n",
        "\n",
        "        return entity_vectors[0]"
      ],
      "metadata": {
        "id": "EeA3WTSJ3EPu"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        user, pos_item, neg_item, pseudo_item = batch\n",
        "        user = user.to(device)\n",
        "        pos_item = pos_item.to(device)\n",
        "        neg_item = neg_item.to(device)\n",
        "        pseudo_item = pseudo_item.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pos_scores = model(user, pos_item)\n",
        "        neg_scores = model(user, neg_item)\n",
        "        pseudo_scores = model(user, pseudo_item)\n",
        "\n",
        "        loss = kgpl_loss(pos_scores, neg_scores, pseudo_scores)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "Hwzy_8Rs3EV1"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = KGPL_Dataset.datasets['train']\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=4,  # optional for faster loading\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = KGPLStudent(model_cfg, 1872, 9366, 3846, train_dataset.adj_entity, train_dataset.adj_relation, train_dataset.path_list_dict, name='student').to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=model_cfg.optimize['lr'])\n",
        "\n",
        "# Train\n",
        "for epoch in tqdm(range(10)):\n",
        "    loss = train_one_epoch(model, train_loader, optimizer, device)\n",
        "    print(f\"Epoch {epoch} | Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "jkRFJM3d3EZw",
        "outputId": "b9120d2c-3165-4b81-bcea-a61bf8490d9d"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]<ipython-input-126-d28979224a6c>:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return user, torch.tensor(pos_item), torch.tensor(neg_item), torch.tensor(pseudo_label)\n",
            "<ipython-input-126-d28979224a6c>:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return user, torch.tensor(pos_item), torch.tensor(neg_item), torch.tensor(pseudo_label)\n",
            "<ipython-input-126-d28979224a6c>:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return user, torch.tensor(pos_item), torch.tensor(neg_item), torch.tensor(pseudo_label)\n",
            "<ipython-input-126-d28979224a6c>:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return user, torch.tensor(pos_item), torch.tensor(neg_item), torch.tensor(pseudo_label)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Something went wrong.\n",
            "Something went wrong.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "Caught IndexError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-126-d28979224a6c>\", line 175, in __getitem__\n    pos_item = self.sample_positive(user)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-126-d28979224a6c>\", line 156, in sample_positive\n    return random.choice(choice_seq)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/random.py\", line 373, in choice\n    raise IndexError('Cannot choose from an empty sequence')\nIndexError: Cannot choose from an empty sequence\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-4be10ded29f4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch} | Loss: {loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-132-4a061aef9c36>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpseudo_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-126-d28979224a6c>\", line 175, in __getitem__\n    pos_item = self.sample_positive(user)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-126-d28979224a6c>\", line 156, in sample_positive\n    return random.choice(choice_seq)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/random.py\", line 373, in choice\n    raise IndexError('Cannot choose from an empty sequence')\nIndexError: Cannot choose from an empty sequence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YGMKQBPW3EdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-chrW8l13Eg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vp-VgrYY3EkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "90OkLfmo3EoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iq3EbOm73Erq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-zLWbCyo3Evf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oog4PE1w3Ezb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MCX4VjNg3E3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vYguKe0i3E7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_mini_batch_pl(users):\n",
        "        \"\"\"\n",
        "        Create pseudo-labelled instances for users\n",
        "        \"\"\"\n",
        "        pl_users, pl_items = [], []\n",
        "        ind = 0\n",
        "        cands, freq_F = self.item_freq # what's item_freq?\n",
        "        while True:\n",
        "            u = users[ind % len(users)]\n",
        "            ind += 1\n",
        "            if u in self.item_dist_dict and len(self.item_dist_dict[u][0]) != 0:\n",
        "                udst, F = self.item_dist_dict[u]\n",
        "                i = udst[np.searchsorted(F, random.random())]\n",
        "            else:\n",
        "                while True:\n",
        "                    i = cands[np.searchsorted(freq_F, random.random())]\n",
        "                    if i not in self.user_seed_dict[u]:\n",
        "                        break\n",
        "            pl_users.append(u)\n",
        "            pl_items.append(i)\n",
        "            if len(pl_users) == len(users):\n",
        "                break\n",
        "\n",
        "        pl_users_pad = list(pl_users) + [0] * (self.batch_size - len(pl_users))\n",
        "        pl_items_pad = list(pl_items) + [0] * (self.batch_size - len(pl_items))\n",
        "        pl_labels_pad = sess.run(\n",
        "            self.scores_normalized,\n",
        "            feed_dict={\n",
        "                self.user_indices: pl_users_pad,\n",
        "                self.item_indices: pl_items_pad,\n",
        "                self.dropout_rate: 0.0,\n",
        "            },\n",
        "        )\n",
        "        pl_users = pl_users_pad[: len(pl_users)]\n",
        "        pl_items = pl_items_pad[: len(pl_items)]\n",
        "        pl_labels = pl_labels_pad[: len(pl_users)]\n",
        "        return pl_users, pl_items, pl_labels"
      ],
      "metadata": {
        "id": "wrt8vP7SP2fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j5n7ISOfP2YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mbiB_wXBP2Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_neighbors(self, seeds):\n",
        "        seeds = tf.expand_dims(seeds, axis=1)\n",
        "        entities = [seeds]\n",
        "        relations = []\n",
        "        for i in range(self.cfg.n_iter):\n",
        "            neighbor_entities = tf.reshape(\n",
        "                tf.gather(self.adj_entity, entities[i]), [self.batch_size, -1]\n",
        "            )\n",
        "            neighbor_relations = tf.reshape(\n",
        "                tf.gather(self.adj_relation, entities[i]), [self.batch_size, -1]\n",
        "            )\n",
        "            entities.append(neighbor_entities)\n",
        "            relations.append(neighbor_relations)\n",
        "        return entities, relations"
      ],
      "metadata": {
        "id": "rDBhuei8DNr7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}