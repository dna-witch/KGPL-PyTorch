{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nISq-HZfQx5",
        "outputId": "37d5583e-fc5b-45ef-9173-9dabcffd7a9d"
      },
      "outputs": [],
      "source": [
        "# %pip install hydra-core==0.11.3\n",
        "# %pip install omegaconf==1.4.1\n",
        "# %pip install loguru==0.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-2fdXCTbWj2l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wfnXVLgSZG8",
        "outputId": "0e1f8e10-c9ee-49d5-96a8-d00b13f5cbc7"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/riktor/KGPL/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyX4r9AgU1Md",
        "outputId": "46eaaab4-b11b-4ada-99aa-e7ece43a6a99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading item index to entity id file: data/movie/item_index2entity_id.txt ...\n",
            "reading rating file ...\n",
            "converting rating file ...\n",
            "number of users: 6040\n",
            "number of items: 2347\n",
            "converting kg file ...\n",
            "number of entities (containing items): 7008\n",
            "number of relations: 7\n",
            "done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shak-\\OneDrive\\Desktop\\myProjects\\KGPL-PyTorch\\KGPL\\preprocess\\make_path_list.py:107: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @hydra.main(config_path=\"../conf/preprocess.yaml\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\shak-\\OneDrive\\Desktop\\myProjects\\KGPL-PyTorch\\KGPL\\preprocess\\make_path_list.py\", line 157, in <module>\n",
            "    main()\n",
            "  File \"c:\\Users\\shak-\\anaconda3\\envs\\kgpl\\lib\\site-packages\\hydra\\main.py\", line 94, in decorated_main\n",
            "    _run_hydra(\n",
            "  File \"c:\\Users\\shak-\\anaconda3\\envs\\kgpl\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 335, in _run_hydra\n",
            "    validate_config_path(config_path)\n",
            "  File \"c:\\Users\\shak-\\anaconda3\\envs\\kgpl\\lib\\site-packages\\hydra\\core\\utils.py\", line 293, in validate_config_path\n",
            "    raise ValueError(msg)\n",
            "ValueError: Using config_path to specify the config name is not supported, specify the config name via config_name.\n",
            "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/config_path_changes\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading item index to entity id file: data/music/item_index2entity_id.txt ...\n",
            "reading rating file ...\n",
            "converting rating file ...\n",
            "number of users: 1872\n",
            "number of items: 3846\n",
            "converting kg file ...\n",
            "number of entities (containing items): 9366\n",
            "number of relations: 60\n",
            "done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shak-\\OneDrive\\Desktop\\myProjects\\KGPL-PyTorch\\KGPL\\preprocess\\make_path_list.py:107: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @hydra.main(config_path=\"../conf/preprocess.yaml\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\shak-\\OneDrive\\Desktop\\myProjects\\KGPL-PyTorch\\KGPL\\preprocess\\make_path_list.py\", line 157, in <module>\n",
            "    main()\n",
            "  File \"c:\\Users\\shak-\\anaconda3\\envs\\kgpl\\lib\\site-packages\\hydra\\main.py\", line 94, in decorated_main\n",
            "    _run_hydra(\n",
            "  File \"c:\\Users\\shak-\\anaconda3\\envs\\kgpl\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 335, in _run_hydra\n",
            "    validate_config_path(config_path)\n",
            "  File \"c:\\Users\\shak-\\anaconda3\\envs\\kgpl\\lib\\site-packages\\hydra\\core\\utils.py\", line 293, in validate_config_path\n",
            "    raise ValueError(msg)\n",
            "ValueError: Using config_path to specify the config name is not supported, specify the config name via config_name.\n",
            "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/config_path_changes\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading item index to entity id file: data/book/item_index2entity_id.txt ...\n",
            "reading rating file ...\n",
            "converting rating file ...\n",
            "number of users: 17860\n",
            "number of items: 14967\n",
            "converting kg file ...\n",
            "number of entities (containing items): 77903\n",
            "number of relations: 25\n",
            "done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shak-\\OneDrive\\Desktop\\myProjects\\KGPL-PyTorch\\KGPL\\preprocess\\make_path_list.py:107: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @hydra.main(config_path=\"../conf/preprocess.yaml\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\shak-\\OneDrive\\Desktop\\myProjects\\KGPL-PyTorch\\KGPL\\preprocess\\make_path_list.py\", line 157, in <module>\n",
            "    main()\n",
            "  File \"c:\\Users\\shak-\\anaconda3\\envs\\kgpl\\lib\\site-packages\\hydra\\main.py\", line 94, in decorated_main\n",
            "    _run_hydra(\n",
            "  File \"c:\\Users\\shak-\\anaconda3\\envs\\kgpl\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 335, in _run_hydra\n",
            "    validate_config_path(config_path)\n",
            "  File \"c:\\Users\\shak-\\anaconda3\\envs\\kgpl\\lib\\site-packages\\hydra\\core\\utils.py\", line 293, in validate_config_path\n",
            "    raise ValueError(msg)\n",
            "ValueError: Using config_path to specify the config name is not supported, specify the config name via config_name.\n",
            "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/config_path_changes\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!cd KGPL &&\\\n",
        "python preprocess/preprocess.py -d \"movie\" &&\\\n",
        "python preprocess/make_path_list.py lp_depth=6 dataset=music kg_path=data/music/kg_final.npy rating_path=data/music/ratings_final.npy num_neighbor_samples=32\n",
        "\n",
        "!cd KGPL &&\\\n",
        "python preprocess/preprocess.py -d \"music\" &&\\\n",
        "python preprocess/make_path_list.py lp_depth=6 dataset=music kg_path=data/music/kg_final.npy rating_path=data/music/ratings_final.npy num_neighbor_samples=32\n",
        "\n",
        "!cd KGPL &&\\\n",
        "python preprocess/preprocess.py -d \"book\" &&\\\n",
        "python preprocess/make_path_list.py lp_depth=6 dataset=book kg_path=data/book/kg_final.npy rating_path=data/book/ratings_final.npy num_neighbor_samples=8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pQlrACnVm6ko"
      },
      "outputs": [],
      "source": [
        "# from KGPL.models.kgpl import KGPL_COT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcojKL0Fpii5",
        "outputId": "0c0ae223-ee4c-4fc3-d6bf-2d188e9f4679"
      },
      "outputs": [],
      "source": [
        "# torch.unique(torch.Tensor([1,2,3]), return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UhZOnz55y5tT"
      },
      "outputs": [],
      "source": [
        "class KGPL_Config():\n",
        "  '''\n",
        "  KGPL model configuration for each dataset.\n",
        "  '''\n",
        "  def __init__(self, dataset_name:str, model_type:str, neighbor_sample_size:int, dropout_rate:float, emb_dim:int, n_iter:int, plabel:dict, optimize:dict, log:dict, evaluate:dict, model:dict):\n",
        "    self.dataset_name = dataset_name\n",
        "    self.model_type = model_type\n",
        "    self.neighbor_sample_size = neighbor_sample_size\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.emb_dim = emb_dim\n",
        "    self.n_iter = n_iter\n",
        "    self.plabel = plabel\n",
        "    self.optimize = optimize\n",
        "    self.log = log\n",
        "    self.evaluate = evaluate\n",
        "    self.model = model\n",
        "\n",
        "\n",
        "\n",
        "def build_user_train_dict_from_tensor(ratings):\n",
        "    user_train = {}\n",
        "    for i in range(ratings.size(0)):\n",
        "        user, item, rating = ratings[i]\n",
        "        if rating >= 1:  # or whatever you define as \"positive\" interaction\n",
        "            user = user.item()\n",
        "            item = item.item()\n",
        "            if user not in user_train:\n",
        "                user_train[user] = []\n",
        "            user_train[user].append(item)\n",
        "    return user_train\n",
        "\n",
        "# def compute_reachable_items_torch(args_list):\n",
        "def compute_reachable_items_nodist(seed_items, kg):\n",
        "    source = kg[:,0]\n",
        "    target = kg[:,2]\n",
        "    # print(source)\n",
        "    # print(target)\n",
        "    mask = (source.unsqueeze(1) == seed_items).any(dim=1) | (target.unsqueeze(1) == seed_items).any(dim=1)\n",
        "    connected_edges = kg[mask]\n",
        "    neighbors = torch.cat([connected_edges[:, 0], connected_edges[:, 2]])\n",
        "    neighbors = neighbors[~torch.isin(neighbors, seed_items)]\n",
        "    return torch.unique(neighbors)\n",
        "\n",
        "def compute_reachable_items(users, user_seed_dict, path_dict, item_freq, power):\n",
        "    \"\"\"\n",
        "    For each user, find reachable items via KG paths and compute a pseudo-label distribution.\n",
        "    This is adapted from compute_reachable_items_.\n",
        "    Returns: dict of user -> (item_list, cumulative_probs)\n",
        "    \"\"\"\n",
        "    reachable = {}\n",
        "    for user in users:\n",
        "        seed_items = user_seed_dict.get(user, [])\n",
        "        # Count reachable paths from each seed\n",
        "        dst = Counter()\n",
        "        for item in seed_items:\n",
        "            dst.update(path_dict.get(item, {}))\n",
        "        if not dst:\n",
        "            continue\n",
        "        udst = np.array(list(dst.keys()))\n",
        "        freq = np.array(list(dst.values())) ** power\n",
        "        # Remove seeds from candidates\n",
        "        mask = ~np.isin(udst, list(seed_items))\n",
        "        udst = udst[mask]\n",
        "        freq = freq[mask]\n",
        "        # Add 'unreachable' items with small weight\n",
        "        all_items = np.array(list(item_freq.keys()))\n",
        "        unreachable = np.setdiff1d(all_items, udst, assume_unique=True)\n",
        "        freq = np.concatenate([freq, np.ones(len(unreachable)) * 0.5])\n",
        "        udst = np.concatenate([udst, unreachable])\n",
        "        # Sort and make CDF\n",
        "        order = np.argsort(freq)\n",
        "        udst = udst[order]; freq = freq[order]\n",
        "        cdf = (freq / freq.sum()).cumsum()\n",
        "        reachable[user] = (udst.tolist(), cdf.tolist())\n",
        "    return reachable\n",
        "\n",
        "def set_item_candidates(\n",
        "        self, n_user, n_item, train_data, eval_data, path_list_dict\n",
        "    ):\n",
        "        \"\"\"Construct the sampling distrbiutions for negative/pseudo-labelled instances for each user\n",
        "        \"\"\"\n",
        "        all_users = tuple(set(train_data[:, 0]))\n",
        "        self.all_users = all_users\n",
        "\n",
        "        self.n_item = n_item\n",
        "        self.all_items = set(range(n_item))\n",
        "        self.neg_c_dict_user = self._build_freq_dict(\n",
        "            np.concatenate([train_data[:, 0], eval_data[:, 0]]), self.all_users\n",
        "        )\n",
        "        self.neg_c_dict_item = self._build_freq_dict(\n",
        "            np.concatenate([train_data[:, 1], eval_data[:, 1]]), self.all_items\n",
        "        )\n",
        "\n",
        "        item_cands = tuple(self.neg_c_dict_item.keys())\n",
        "        F = np.array(tuple(self.neg_c_dict_item.values())) ** self.cfg.plabel.neg_pn\n",
        "        sort_inds = np.argsort(F)\n",
        "        item_cands = [item_cands[i] for i in sort_inds]\n",
        "        F = F[sort_inds]\n",
        "        F = (F / F.sum()).cumsum()\n",
        "        self.item_freq = (item_cands, F)\n",
        "\n",
        "        for u, i in tqdm(train_data[:, 0:2]):\n",
        "            self.user_seed_dict[u].add(i)\n",
        "\n",
        "        path = hydra.utils.to_absolute_path(self.cfg.reachable_items_path)\n",
        "        logger.info(\"calculating reachable items for users\")\n",
        "        self._setup_dst_dict(path_list_dict)\n",
        "        item_dist_dict = {}\n",
        "        src_itr = map(\n",
        "            lambda iu: (\n",
        "                all_users[iu],\n",
        "                tuple(self.user_seed_dict[all_users[iu]]),\n",
        "                self.dst_dict,\n",
        "                self.neg_c_dict_item,\n",
        "                self.cfg.plabel.pl_pn,\n",
        "            ),\n",
        "            range(len(all_users)),\n",
        "        )\n",
        "        grouped = grouper(self.cfg.plabel.chunk_size, src_itr, squash=set([2, 3]))\n",
        "        with mp.Pool(self.cfg.plabel.par) as pool:\n",
        "            for idd in pool.imap_unordered(compute_reachable_items_, grouped):\n",
        "                item_dist_dict.update(idd)\n",
        "        self.item_dist_dict = item_dist_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Need to troubleshoot `compute_reachable_items`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PPnbQROj5vLV"
      },
      "outputs": [],
      "source": [
        "# # def compute_reachable_items(ratings):\n",
        "# #     \"\"\"\n",
        "# #     ratings: torch.Tensor of shape (N, 3), where each row is (user, item, rating)\n",
        "# #     \"\"\"\n",
        "# #     reachable_items = dict()\n",
        "\n",
        "# #     for user, item, rating in ratings.tolist():\n",
        "# #         if rating == 1:  # positive interaction\n",
        "# #             if user not in reachable_items:\n",
        "# #                 reachable_items[user.item()] = set()\n",
        "# #             reachable_items[user.item()].add(item.item())\n",
        "\n",
        "# #     return reachable_items\n",
        "\n",
        "# def compute_reachable_items(users, user_seed_dict, path_dict, item_freq, power):\n",
        "#     \"\"\"\n",
        "#     For each user, find reachable items via KG paths and compute a pseudo-label distribution.\n",
        "#     This is adapted from compute_reachable_items_.\n",
        "#     Returns: dict of user -> (item_list, cumulative_probs)\n",
        "#     \"\"\"\n",
        "#     reachable = {}\n",
        "#     for user in users:\n",
        "#         seed_items = user_seed_dict.get(user, [])\n",
        "#         # Count reachable paths from each seed\n",
        "#         dst = Counter()\n",
        "#         for item in seed_items:\n",
        "#             dst.update(path_dict.get(item, {}))\n",
        "#         if not dst:\n",
        "#             continue\n",
        "#         udst = np.array(list(dst.keys()))\n",
        "#         freq = np.array(list(dst.values())) ** power\n",
        "#         # Remove seeds from candidates\n",
        "#         mask = ~np.isin(udst, list(seed_items))\n",
        "#         udst = udst[mask]\n",
        "#         freq = freq[mask]\n",
        "#         # Add 'unreachable' items with small weight\n",
        "#         all_items = np.array(list(item_freq.keys()))\n",
        "#         unreachable = np.setdiff1d(all_items, udst, assume_unique=True)\n",
        "#         freq = np.concatenate([freq, np.ones(len(unreachable)) * 0.5])\n",
        "#         udst = np.concatenate([udst, unreachable])\n",
        "#         # Sort and make CDF\n",
        "#         order = np.argsort(freq)\n",
        "#         udst = udst[order]; freq = freq[order]\n",
        "#         cdf = (freq / freq.sum()).cumsum()\n",
        "#         reachable[user] = (udst.tolist(), cdf.tolist())\n",
        "#     return reachable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "python: can't open file 'c:\\\\Users\\\\shak-\\\\OneDrive\\\\Desktop\\\\myProjects\\\\KGPL-PyTorch\\\\preprocess\\\\preprocess.py': [Errno 2] No such file or directory\n",
            "python: can't open file 'c:\\\\Users\\\\shak-\\\\OneDrive\\\\Desktop\\\\myProjects\\\\KGPL-PyTorch\\\\preprocess\\\\make_path_list.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python preprocess/preprocess.py -d music\n",
        "!python preprocess/make_path_list.py lp_depth=6 dataset=music kg_path=data/music/kg_final.npy rating_path=data/music/ratings_final.npy num_neighbor_samples=32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb4mI82_TQZ3"
      },
      "outputs": [],
      "source": [
        "class KGPL_Dataset(Dataset):\n",
        "  '''\n",
        "  Custom dataset class which includes all datasets and parameters per model.\n",
        "  Specified under \"data\" directory\n",
        "  '''\n",
        "\n",
        "  base_data_path = 'KGPL/data/'\n",
        "\n",
        "  def readjust_counts(self):\n",
        "    unique_users = torch.unique(self.ratings[:,0], return_counts=True)\n",
        "    self.users = unique_users[0]\n",
        "    self.n_user = unique_users[1][0].item()\n",
        "    self.n_item = torch.unique(self.ratings[:,1]).numel()\n",
        "    self.reachable_items = compute_reachable_items_nodist(self.users, self.ratings)\n",
        "\n",
        "  def __init__(self,dataset_name:str):\n",
        "    self.dataset_name = dataset_name\n",
        "    self.entity_adj = torch.from_numpy(np.load(self.base_data_path + self.dataset_name + '/adj_entity_6_32.npy'))\n",
        "    self.relation_adj = torch.from_numpy(np.load(self.base_data_path + self.dataset_name + '/adj_relation_6_32.npy'))\n",
        "    self.ratings = torch.from_numpy(np.load(self.base_data_path + self.dataset_name + '/ratings_final.npy'))\n",
        "    self.path_list_dict = pickle.load(open(self.base_data_path + self.dataset_name + '/path_list_6_32.pkl', 'rb'))\n",
        "    #kg is a 3-column matrix of undirected relations: (head, relation, tail)\n",
        "    self.kg = np.load(self.base_data_path + self.dataset_name + '/kg_final.npy')\n",
        "    self.readjust_counts()\n",
        "\n",
        "  def __len__(self):\n",
        "    # doing interactions\n",
        "    return self.n_user\n",
        "\n",
        "  def sample_positive(self, user):\n",
        "        return random.choice(self.user_train[user])\n",
        "\n",
        "  def sample_negative(self, user):\n",
        "      seen = set(self.user_train[user])\n",
        "      while True:\n",
        "          item = random.randint(0, self.n_item - 1)\n",
        "          if item not in seen:\n",
        "              return item\n",
        "\n",
        "  # def sample_pseudo_label(self, user):\n",
        "  #       F = compute_reachable_items_torch(args_list)\n",
        "  #       # udst, F = self.reachable_items[user]\n",
        "  #       r = random.random()\n",
        "  #       idx = torch.searchsorted(F, r, right=True).item()\n",
        "  #       return udst[idx].item()\n",
        "  def sample_pseudo_label(self, user):\n",
        "        udst, F = self.reachable_items[user]\n",
        "        r = random.random()\n",
        "        idx = torch.searchsorted(F, r, right=True).item()\n",
        "        return udst[idx].item()\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    if self.train_set:\n",
        "      user = self.users[idx]\n",
        "      pos_item = self.sample_positive(idx)\n",
        "      neg_item = self.sample_negative(idx)\n",
        "      pseudo_label = self.sample_pseudo_label(user)\n",
        "      return user, pos_item, neg_item, pseudo_label\n",
        "    else:\n",
        "      # still need to implement\n",
        "      return None\n",
        "\n",
        "  def _split_data(self, split_ratio=0.2):\n",
        "    #split dataset\n",
        "    n_ratings = len(self.ratings)\n",
        "    split_indices = torch.randperm(n_ratings)[:int(n_ratings * split_ratio)]\n",
        "    splitted_data = self.ratings[split_indices]\n",
        "    rest_data = self.ratings[~torch.isin(torch.arange(n_ratings), split_indices)]\n",
        "    #create new objects\n",
        "    splitted_dataset, rest_dataset = deepcopy(self), deepcopy(self)\n",
        "    splitted_dataset.ratings = splitted_data\n",
        "    rest_dataset.ratings = rest_data\n",
        "    splitted_dataset.readjust_counts()\n",
        "    rest_dataset.readjust_counts()\n",
        "\n",
        "    return rest_dataset, splitted_dataset\n",
        "\n",
        "  def train_val_test_split(self):\n",
        "    exp_dataset, test = self._split_data()\n",
        "    train, val = exp_dataset._split_data()\n",
        "    n_user = torch.unique(self.ratings[:,0]).numel()\n",
        "    n_item = torch.unique(self.ratings[:,1]).numel()\n",
        "    #readjust counts\n",
        "    train.readjust_counts()\n",
        "    val.readjust_counts()\n",
        "    test.readjust_counts()\n",
        "    train.user_train = build_user_train_dict_from_tensor(train.ratings)\n",
        "    train.train_set = True\n",
        "    return (n_user, n_item, train, val, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Rw0SB_VMHkKC"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'KGPL/data/music/adj_entity_6_32.npy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mKGPL_Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmusic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtrain_val_test_split()\n\u001b[0;32m      3\u001b[0m cfg \u001b[38;5;241m=\u001b[39m KGPL_Config(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmusic\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKGPL_COT\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_iter\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneighbor_sample_size\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m32\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0.5\u001b[39m}\n\u001b[0;32m     15\u001b[0m )\n",
            "Cell \u001b[1;32mIn[18], line 18\u001b[0m, in \u001b[0;36mKGPL_Dataset.__init__\u001b[1;34m(self, dataset_name)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,dataset_name:\u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     17\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name \u001b[38;5;241m=\u001b[39m dataset_name\n\u001b[1;32m---> 18\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentity_adj \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_data_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/adj_entity_6_32.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelation_adj \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_data_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/adj_relation_6_32.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     20\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mratings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_data_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/ratings_final.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
            "File \u001b[1;32mc:\\Users\\shak-\\anaconda3\\envs\\kgpl\\lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'KGPL/data/music/adj_entity_6_32.npy'"
          ]
        }
      ],
      "source": [
        "data = KGPL_Dataset('music').train_val_test_split()\n",
        "\n",
        "cfg = KGPL_Config(\n",
        "    'music',\n",
        "    'KGPL_COT',\n",
        "    neighbor_sample_size=32,\n",
        "    dropout_rate=0.5,\n",
        "    emb_dim=64,\n",
        "    n_iter=1,\n",
        "    plabel={},\n",
        "    optimize={'iter_per_epoch':100, 'lr': 3e-3, 'batch_size':3333},\n",
        "    log={'show_loss':True},\n",
        "    evaluate={'user_num_topk':1000},\n",
        "    model={'n_iter':1, 'neighbor_sample_size':32, 'dropout_rate':0.5}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "CK8U4-vhmFlX"
      },
      "outputs": [],
      "source": [
        "def kgpl_loss(pos_scores, neg_scores, pseudo_scores):\n",
        "    # BCE loss like TensorFlow version\n",
        "    pos_labels = torch.ones_like(pos_scores)\n",
        "    neg_labels = torch.zeros_like(neg_scores)\n",
        "    pseudo_labels = torch.ones_like(pseudo_scores)\n",
        "\n",
        "    loss = F.binary_cross_entropy_with_logits(pos_scores, pos_labels) + \\\n",
        "           F.binary_cross_entropy_with_logits(neg_scores, neg_labels) + \\\n",
        "           F.binary_cross_entropy_with_logits(pseudo_scores, pseudo_labels)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "MJWzXvdwmFgA"
      },
      "outputs": [],
      "source": [
        "class SumAggregatorWithDropout(nn.Module):\n",
        "    def __init__(self, emb_dim, dropout_rate, activation, cfg):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(emb_dim * 2, emb_dim)\n",
        "        self.activation = activation\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, self_vectors, neighbor_vectors, neighbor_relations, user_embeddings):\n",
        "        # self_vectors: [batch_size, emb_dim]\n",
        "        # neighbor_vectors: [batch_size, n_neighbors, emb_dim]\n",
        "        neighbor_mean = neighbor_vectors.mean(dim=1)  # [batch_size, emb_dim]\n",
        "        out = torch.cat([self_vectors, neighbor_mean], dim=-1)  # [batch_size, emb_dim * 2]\n",
        "        out = self.linear(out)\n",
        "        out = self.dropout(out)\n",
        "        return self.activation(out)\n",
        "\n",
        "\n",
        "class KGPLStudent(nn.Module):\n",
        "    def __init__(self, cfg, n_user, n_entity, n_relation, adj_entity, adj_relation, path_list_dict, name, eval_mode=False):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.name = name\n",
        "        self.n_user = n_user\n",
        "        self.n_entity = n_entity\n",
        "        self.n_relation = n_relation\n",
        "        self.batch_size = cfg.optimize['batch_size']\n",
        "        self.adj_entity = adj_entity\n",
        "        self.adj_relation = adj_relation\n",
        "        self.path_list_dict = path_list_dict\n",
        "        self.eval_mode = eval_mode\n",
        "\n",
        "        self.user_emb_matrix = nn.Embedding(n_user, cfg.emb_dim)\n",
        "        self.entity_emb_matrix = nn.Embedding(n_entity, cfg.emb_dim)\n",
        "        self.relation_emb_matrix = nn.Embedding(n_relation, cfg.emb_dim)\n",
        "\n",
        "        self.aggregators = nn.ModuleList([\n",
        "            SumAggregatorWithDropout(cfg.emb_dim, cfg.dropout_rate, activation=nn.Tanh(), cfg=cfg)\n",
        "            if i == cfg.n_iter - 1 else\n",
        "            SumAggregatorWithDropout(cfg.emb_dim, cfg.dropout_rate, activation=nn.LeakyReLU(), cfg=cfg)\n",
        "            for i in range(cfg.n_iter)\n",
        "        ])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.user_emb_matrix.weight)\n",
        "        nn.init.xavier_uniform_(self.entity_emb_matrix.weight)\n",
        "        nn.init.xavier_uniform_(self.relation_emb_matrix.weight)\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embeds = self.user_emb_matrix(user_indices)\n",
        "        item_embeds = self.get_item_embeddings(item_indices)\n",
        "        scores = (user_embeds * item_embeds).sum(dim=-1)\n",
        "        return scores\n",
        "\n",
        "    def get_item_embeddings(self, item_indices):\n",
        "        entities = [item_indices.unsqueeze(1)]\n",
        "        relations = []\n",
        "        for i in range(self.cfg.n_iter):\n",
        "            neighbor_entities = self.adj_entity[entities[-1]].view(item_indices.size(0), -1)\n",
        "            neighbor_relations = self.adj_relation[entities[-1]].view(item_indices.size(0), -1)\n",
        "            entities.append(neighbor_entities)\n",
        "            relations.append(neighbor_relations)\n",
        "\n",
        "        entity_vectors = [self.entity_emb_matrix(e) for e in entities]\n",
        "        relation_vectors = [self.relation_emb_matrix(r) for r in relations]\n",
        "\n",
        "        for i in range(self.cfg.n_iter):\n",
        "            new_vectors = []\n",
        "            for hop in range(self.cfg.n_iter - i):\n",
        "                batch_size, neighbor_size, emb_dim = entity_vectors[hop+1].size(0), entity_vectors[hop+1].size(1) // self.cfg.model.neighbor_sample_size, entity_vectors[hop+1].size(2)\n",
        "                neighbor_vecs = entity_vectors[hop+1].view(batch_size, neighbor_size, self.cfg.model.neighbor_sample_size, emb_dim)\n",
        "                relation_vecs = relation_vectors[hop].view(batch_size, neighbor_size, self.cfg.model.neighbor_sample_size, emb_dim)\n",
        "                vector = self.aggregators[i](\n",
        "                    self_vectors=entity_vectors[hop],\n",
        "                    neighbor_vectors=neighbor_vecs,\n",
        "                    neighbor_relations=relation_vecs,\n",
        "                    user_embeddings=None  # optional\n",
        "                )\n",
        "                new_vectors.append(vector)\n",
        "            entity_vectors = new_vectors\n",
        "\n",
        "        return entity_vectors[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "Z6gdtS2YmFak"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        user, pos_item, neg_item, pseudo_item = batch\n",
        "        user = user.to(device)\n",
        "        pos_item = pos_item.to(device)\n",
        "        neg_item = neg_item.to(device)\n",
        "        pseudo_item = pseudo_item.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pos_scores = model(user, pos_item)\n",
        "        neg_scores = model(user, neg_item)\n",
        "        pseudo_scores = model(user, pseudo_item)\n",
        "\n",
        "        loss = kgpl_loss(pos_scores, neg_scores, pseudo_scores)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "YAvU2ClamFUw",
        "outputId": "0e597a7d-5d51-46a4-f3c9-a679bc9e0bb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-252-bbffcb7c2058>\", line 57, in __getitem__\n    pseudo_label = self.sample_pseudo_label(user)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-252-bbffcb7c2058>\", line 47, in sample_pseudo_label\n    udst, F = self.reachable_items[user]\n              ~~~~~~~~~~~~~~~~~~~~^^^^^^\nIndexError: index 9 is out of bounds for dimension 0 with size 0\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-261-f46f4d5f6233>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch} | Loss: {loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-260-4a061aef9c36>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpseudo_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-252-bbffcb7c2058>\", line 57, in __getitem__\n    pseudo_label = self.sample_pseudo_label(user)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-252-bbffcb7c2058>\", line 47, in sample_pseudo_label\n    udst, F = self.reachable_items[user]\n              ~~~~~~~~~~~~~~~~~~~~^^^^^^\nIndexError: index 9 is out of bounds for dimension 0 with size 0\n"
          ]
        }
      ],
      "source": [
        "train_dataset = data[2]\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=4,  # optional for faster loading\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = KGPLStudent(cfg, 1872, 3846, 60, train_dataset.entity_adj, train_dataset.relation_adj, train_dataset.path_list_dict, name='student').to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.optimize['lr'])\n",
        "\n",
        "# Train\n",
        "for epoch in tqdm(range(10)):\n",
        "    loss = train_one_epoch(model, train_loader, optimizer, device)\n",
        "    print(f\"Epoch {epoch} | Loss: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8waBJjGUmFOX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVPpzouPmFEX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFZ3W3qxm74R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-px9MxVm79p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek2btkFXm8BQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy7TVm4Lm8FE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo_Eur88m8JS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW8pTa-tm8Ok"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGKWBFJhToVV"
      },
      "source": [
        "# JUNK BELOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "E9bBeRhAUbBc"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "\n",
        "def grouper(n, iterable, squash=None):\n",
        "    it = iter(iterable)\n",
        "    while True:\n",
        "        if squash:\n",
        "            chunk = [\n",
        "                [None if (j != 0 and i in squash) else el[i] for i in range(len(el))]\n",
        "                for j, el in enumerate(itertools.islice(it, n))\n",
        "            ]\n",
        "        else:\n",
        "            chunk = list(itertools.islice(it, n))\n",
        "\n",
        "        if not chunk:\n",
        "            return\n",
        "        elif len(chunk) != n:\n",
        "            chunk += [None] * (n - len(chunk))\n",
        "        yield chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "cr3MmOjeTnib"
      },
      "outputs": [],
      "source": [
        "def compute_reachable_items_(args_list):\n",
        "    \"\"\"Construct the sampling distributions based on paths in KG.\n",
        "    Args:\n",
        "        args_list: list of list of arguments. Each arguments' list must contains;\n",
        "        (1) user_id;\n",
        "        (2) user's interacted item ids (seed items);\n",
        "        (3) item-to-(item, #paths) dict found in the BFS (start and end points of some paths);\n",
        "        (4) item-to-frequency dict;\n",
        "        (5) power coefficient to control the skewness of sampling distributions\n",
        "    Returns:\n",
        "        dict in which (key, value) = (item list, np.array of sampling distribution).\n",
        "        sampling distribution is transformed to CDF for fast sampling.\n",
        "    \"\"\"\n",
        "    idd = {}\n",
        "    _, _, dst_dict, item_freq, pn = args_list[0]\n",
        "    for args in args_list:\n",
        "        if args is None:\n",
        "            continue\n",
        "        user, seed_items, _, _, _ = args\n",
        "\n",
        "        # Collect user's reachable items with the number of reachable paths\n",
        "        dst = Counter()\n",
        "        for item in seed_items:\n",
        "            if item in dst_dict:\n",
        "                dst += dst_dict[item]\n",
        "\n",
        "        if len(dst) != 0:\n",
        "            # Unique reachable items for the user\n",
        "            udst = np.array(tuple(dst.keys()))\n",
        "\n",
        "            # Histogram of paths with power transform\n",
        "            F = np.array(tuple(dst.values())) ** pn\n",
        "\n",
        "            # Remove the seed (positve) items\n",
        "            inds = ~np.isin(udst, seed_items)\n",
        "            udst = udst[inds]\n",
        "            F = F[inds]\n",
        "\n",
        "            # Compute unreachable items and concat those to the end of item lists\n",
        "            udst = set(udst)\n",
        "            unreachable_items = [i for i in item_freq if i not in udst]\n",
        "            udst = list(udst) + unreachable_items\n",
        "\n",
        "            # For unreachable items, assume 0.5 virtual paths for full support\n",
        "            F = np.concatenate([F, np.ones(len(unreachable_items)) * 0.5])\n",
        "\n",
        "            # Transform histogram to CDF\n",
        "            sort_inds = np.argsort(F)\n",
        "            udst = [udst[i] for i in sort_inds]\n",
        "            F = F[sort_inds]\n",
        "            F = (F / np.sum(F)).cumsum()\n",
        "            idd[user] = (udst, F)\n",
        "    return idd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "m3wm5lTtT3L-"
      },
      "outputs": [],
      "source": [
        "def compute_user_unseen_items_(args_list):\n",
        "    user_unseen_items = {}\n",
        "    all_items, _, _ = args_list[0]\n",
        "    for args in args_list:\n",
        "        if args is None:\n",
        "            continue\n",
        "        _, user, seed_items, _, _ = args\n",
        "        unseen_items = tuple(all_items - seed_items)\n",
        "        user_unseen_items[user] = (unseen_items, None)\n",
        "    return user_unseen_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "pEispFmzkekS"
      },
      "outputs": [],
      "source": [
        "class KaPLMixin:\n",
        "    def __init__(self, cfg, n_entity, n_relation, adj_entity, adj_relation, path_list_dict, eval_mode=False):\n",
        "        self.cfg = cfg\n",
        "        self.n_entity = n_entity\n",
        "        self.n_relation = n_relation\n",
        "        self.user_seed_dict = defaultdict(set)\n",
        "        self.item_dist_dict = {}\n",
        "        self.cand_uinds = None\n",
        "        self.cand_iinds = None\n",
        "\n",
        "    def _build_freq_dict(self, seq, all_candidates):\n",
        "        _freq = Counter(seq)\n",
        "        for i in all_candidates:\n",
        "            if i not in _freq:\n",
        "                _freq[i] += 1\n",
        "        freq = [_freq[i] for i in all_candidates]\n",
        "        return dict(zip(all_candidates, freq))\n",
        "\n",
        "    def set_item_candidates(self, n_user, n_item, train_data, eval_data, path_list_dict):\n",
        "        all_users = tuple(set(train_data[:, 0]))\n",
        "        self.all_users = all_users\n",
        "        self.n_item = n_item\n",
        "        self.all_items = set(range(n_item))\n",
        "\n",
        "        self.neg_c_dict_user = self._build_freq_dict(\n",
        "            np.concatenate([train_data[:, 0], eval_data[:, 0]]), self.all_users\n",
        "        )\n",
        "        self.neg_c_dict_item = self._build_freq_dict(\n",
        "            np.concatenate([train_data[:, 1], eval_data[:, 1]]), self.all_items\n",
        "        )\n",
        "\n",
        "        item_cands = tuple(self.neg_c_dict_item.keys())\n",
        "        F = np.array(list(self.neg_c_dict_item.values())) ** self.cfg.plabel['neg_pn']\n",
        "        sort_inds = np.argsort(F)\n",
        "        item_cands = [item_cands[i] for i in sort_inds]\n",
        "        F = F[sort_inds]\n",
        "        F = F / F.sum()\n",
        "        F = np.cumsum(F)\n",
        "        self.item_freq = (item_cands, F)\n",
        "\n",
        "        for u, i in train_data[:, 0:2]:\n",
        "            self.user_seed_dict[u].add(i)\n",
        "\n",
        "        self._setup_dst_dict(path_list_dict)\n",
        "        item_dist_dict = {}\n",
        "\n",
        "        src_itr = [\n",
        "            (all_users[iu], tuple(self.user_seed_dict[all_users[iu]]), self.dst_dict,\n",
        "             self.neg_c_dict_item, self.cfg.plabel.pl_pn)\n",
        "            for iu in range(len(all_users))\n",
        "        ]\n",
        "\n",
        "        # Using multiprocess (if you want parallelism) - here shown sequentially\n",
        "        for idd in map(compute_reachable_items_, [src_itr[i:i+self.cfg.plabel.chunk_size] for i in range(0, len(src_itr), self.cfg.plabel.chunk_size)]):\n",
        "            item_dist_dict.update(idd)\n",
        "\n",
        "        self.item_dist_dict = item_dist_dict\n",
        "\n",
        "    def _setup_dst_dict(self, path_list_dict):\n",
        "        dst_dict = {}\n",
        "        for item, paths in path_list_dict.items():\n",
        "            dst = []\n",
        "            for p in paths:\n",
        "                dst.append(p[-1])\n",
        "            dst_dict[item] = Counter(dst)\n",
        "        self.dst_dict = dst_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "q47kMAMaksCs"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class KGPLStudent(nn.Module, KaPLMixin):\n",
        "    def __init__(self, cfg, n_user, n_entity, n_relation, adj_entity, adj_relation, path_list_dict, name, eval_mode=False):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.cfg = cfg\n",
        "        self.n_user = n_user\n",
        "        self.n_entity = n_entity\n",
        "        self.n_relation = n_relation\n",
        "        self.batch_size = cfg.optimize.batch_size\n",
        "\n",
        "        # Embedding matrices\n",
        "        self.user_emb_matrix = nn.Embedding(n_user, cfg.emb_dim)\n",
        "        self.entity_emb_matrix = nn.Embedding(n_entity, cfg.emb_dim)\n",
        "        self.relation_emb_matrix = nn.Embedding(n_relation, cfg.emb_dim)\n",
        "\n",
        "        # Neighborhood info\n",
        "        self.adj_entity = torch.tensor(adj_entity, dtype=torch.long)\n",
        "        self.adj_relation = torch.tensor(adj_relation, dtype=torch.long)\n",
        "\n",
        "        KaPLMixin.__init__(self, cfg, n_entity, n_relation, adj_entity, adj_relation, path_list_dict, eval_mode)\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embeddings = self.user_emb_matrix(user_indices)\n",
        "        entities, relations = self.get_neighbors(item_indices)\n",
        "        item_embeddings = self.aggregate(entities, relations)\n",
        "        scores = (user_embeddings * item_embeddings).sum(dim=1)\n",
        "        return torch.sigmoid(scores)\n",
        "\n",
        "    def get_neighbors(self, seeds):\n",
        "        entities = [seeds.unsqueeze(1)]\n",
        "        relations = []\n",
        "        for _ in range(self.cfg.n_iter):\n",
        "            neighbor_entities = self.adj_entity[entities[-1]].view(seeds.size(0), -1)\n",
        "            neighbor_relations = self.adj_relation[entities[-1]].view(seeds.size(0), -1)\n",
        "            entities.append(neighbor_entities)\n",
        "            relations.append(neighbor_relations)\n",
        "        return entities, relations\n",
        "\n",
        "    def aggregate(self, entities, relations):\n",
        "        entity_vectors = [self.entity_emb_matrix(entity) for entity in entities]\n",
        "        relation_vectors = [self.relation_emb_matrix(rel) for rel in relations]\n",
        "        for i in range(self.cfg.n_iter):\n",
        "            if i == self.cfg.n_iter - 1:\n",
        "                act = torch.tanh\n",
        "            else:\n",
        "                act = F.leaky_relu\n",
        "            entity_vectors_next_iter = []\n",
        "            for hop in range(self.cfg.n_iter - i):\n",
        "                self_vector = entity_vectors[hop]\n",
        "                neighbor_vector = entity_vectors[hop + 1].view(self_vector.size(0), -1, self.cfg.neighbor_sample_size, self.cfg.emb_dim)\n",
        "                neighbor_relation = relation_vectors[hop].view(self_vector.size(0), -1, self.cfg.neighbor_sample_size, self.cfg.emb_dim)\n",
        "                # Simple sum aggregation\n",
        "                vector = act(self_vector + neighbor_vector.mean(2) + neighbor_relation.mean(2))\n",
        "                entity_vectors_next_iter.append(vector)\n",
        "            entity_vectors = entity_vectors_next_iter\n",
        "        res = entity_vectors[0].view(self.batch_size, self.cfg.emb_dim)\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50eFL91dkr4c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH3dugxJkeTG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bEyIl3vUNIL"
      },
      "outputs": [],
      "source": [
        "class KaPLMixin(object):\n",
        "    def __init__(\n",
        "        self, cfg, n_entity, n_relation, adj_entity, adj_relation, path_list_dict, eval_mode=False\n",
        "    ):\n",
        "        self.user_seed_dict = defaultdict(set)\n",
        "        self.item_dist_dict = {}\n",
        "        self.cand_uinds = None\n",
        "        self.cand_iinds = None\n",
        "        self.n_entity = n_entity\n",
        "        self.n_relation = n_relation\n",
        "\n",
        "    def _build_freq_dict(self, seq, all_candidates):\n",
        "        _freq = Counter(seq)\n",
        "        for i in all_candidates:\n",
        "            if i not in _freq:\n",
        "                _freq[i] += 1\n",
        "        freq = [_freq[i] for i in all_candidates]\n",
        "        return dict(zip(all_candidates, freq))\n",
        "\n",
        "    def set_item_candidates(\n",
        "        self, n_user, n_item, train_data, eval_data, path_list_dict\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Construct the sampling distrbiutions for negative/pseudo-labelled instances for each user\n",
        "        \"\"\"\n",
        "        all_users = tuple(set(train_data[:, 0]))\n",
        "        self.all_users = all_users\n",
        "\n",
        "        self.n_item = n_item\n",
        "        self.all_items = set(range(n_item))\n",
        "        self.neg_c_dict_user = self._build_freq_dict(\n",
        "            np.concatenate([train_data[:, 0], eval_data[:, 0]]), self.all_users\n",
        "        )\n",
        "        self.neg_c_dict_item = self._build_freq_dict(\n",
        "            np.concatenate([train_data[:, 1], eval_data[:, 1]]), self.all_items\n",
        "        )\n",
        "\n",
        "        item_cands = tuple(self.neg_c_dict_item.keys())\n",
        "        F = np.array(tuple(self.neg_c_dict_item.values())) ** self.cfg.plabel.neg_pn\n",
        "        sort_inds = np.argsort(F)\n",
        "        item_cands = [item_cands[i] for i in sort_inds]\n",
        "        F = F[sort_inds]\n",
        "        F = (F / F.sum()).cumsum()\n",
        "        self.item_freq = (item_cands, F)\n",
        "\n",
        "        for u, i in tqdm(train_data[:, 0:2]):\n",
        "            self.user_seed_dict[u].add(i)\n",
        "\n",
        "        # path = hydra.utils.to_absolute_path(self.cfg.reachable_items_path)\n",
        "        print(\"calculating reachable items for users\")\n",
        "        self._setup_dst_dict(path_list_dict)\n",
        "        item_dist_dict = {}\n",
        "        src_itr = map(\n",
        "            lambda iu: (\n",
        "                all_users[iu],\n",
        "                tuple(self.user_seed_dict[all_users[iu]]),\n",
        "                self.dst_dict,\n",
        "                self.neg_c_dict_item,\n",
        "                self.cfg.plabel.pl_pn,\n",
        "            ),\n",
        "            range(len(all_users)),\n",
        "        )\n",
        "        grouped = grouper(self.cfg.plabel.chunk_size, src_itr, squash=set([2, 3]))\n",
        "        with mp.Pool(self.cfg.plabel.par) as pool:\n",
        "            for idd in pool.imap_unordered(compute_reachable_items_, grouped):\n",
        "                item_dist_dict.update(idd)\n",
        "        self.item_dist_dict = item_dist_dict\n",
        "\n",
        "    def _setup_dst_dict(self, path_list_dict):\n",
        "        \"\"\"\n",
        "        Transform path representations:\n",
        "        `list of nodes` to `dictionaly of source to sink (dst_dict)`\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"setup dst dict...\")\n",
        "        dst_dict = {}\n",
        "        for item in tqdm(path_list_dict):\n",
        "            dst = []\n",
        "            paths = path_list_dict[item]\n",
        "            for i, p in enumerate(paths):\n",
        "                dst.append(p[-1])\n",
        "            dst_dict[item] = Counter(dst)\n",
        "        print(\"start updating path info...\")\n",
        "        self.dst_dict = dst_dict\n",
        "        print.info(\"path info updated.\")\n",
        "\n",
        "    # def _get_user_rel_scores(self, sess, users):\n",
        "    def _get_user_rel_scores(self, users)\n",
        "        uembs = self.user_embeddings()\n",
        "\n",
        "        # self.user_indices = users\n",
        "\n",
        "        uembs =\n",
        "\n",
        "        # sess.run(\n",
        "        #     self.user_embeddings, feed_dict={self.user_indices: users, self.dropout_rate: 0.0}\n",
        "        # )  # nu, legth\n",
        "        # rembs = sess.run(self.relation_emb_matrix)  # nr, length\n",
        "        rembs = self.relation_emb_matrix()\n",
        "\n",
        "        return np.dot(uembs, rembs.T)  # nu, nr\n",
        "\n",
        "    def _get_mini_batch_pl(self, sess, users):\n",
        "        \"\"\"\n",
        "        Create pseudo-labelled instances for users\n",
        "        \"\"\"\n",
        "        pl_users, pl_items = [], []\n",
        "        ind = 0\n",
        "        cands, freq_F = self.item_freq\n",
        "        while True:\n",
        "            u = users[ind % len(users)]\n",
        "            ind += 1\n",
        "            if u in self.item_dist_dict and len(self.item_dist_dict[u][0]) != 0:\n",
        "                udst, F = self.item_dist_dict[u]\n",
        "                i = udst[np.searchsorted(F, torch.rand(1).item())] #changed random to torch\n",
        "            else:\n",
        "                while True:\n",
        "                    i = cands[np.searchsorted(freq_F, torch.rand(1).item())] #changed random to torch\n",
        "                    if i not in self.user_seed_dict[u]:\n",
        "                        break\n",
        "            pl_users.append(u)\n",
        "            pl_items.append(i)\n",
        "            if len(pl_users) == len(users):\n",
        "                break\n",
        "\n",
        "        pl_users_pad = list(pl_users) + [0] * (self.batch_size - len(pl_users))\n",
        "        pl_items_pad = list(pl_items) + [0] * (self.batch_size - len(pl_items))\n",
        "\n",
        "        # start taylor added for PyTorch\n",
        "        # self.user_indices = pl_users_pad\n",
        "        # self.item_indices = pl_items_pad\n",
        "        # self.scores_normalized = self._build_model(n_user, n_entity, n_relation)\n",
        "        # end taylor addded pytorch\n",
        "\n",
        "        pl_labels_pad = sess.run(\n",
        "            self.scores_normalized,\n",
        "            feed_dict={\n",
        "                self.user_indices: pl_users_pad,\n",
        "                self.item_indices: pl_items_pad,\n",
        "                self.dropout_rate: 0.0,\n",
        "            },\n",
        "        )\n",
        "        pl_users = pl_users_pad[: len(pl_users)]\n",
        "        pl_items = pl_items_pad[: len(pl_items)]\n",
        "        pl_labels = pl_labels_pad[: len(pl_users)]\n",
        "        return pl_users, pl_items, pl_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLyzfKD1tjnp"
      },
      "source": [
        "## Train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "H6Vu93_7NoT6"
      },
      "outputs": [],
      "source": [
        "class KGPL_COT():\n",
        "  def __init__(self, *args):\n",
        "    pass\n",
        "\n",
        "def topk_settings(*args, **kwargs):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LNn4UQ9auEv",
        "outputId": "fc5d73da-6023-4ff4-c3f5-c4b1b69ea6e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num train records: 27102\n",
            "num adj entities: 9366, num entities: 9366\n",
            "num adj relations: 9366, num relations: 60\n",
            "model type: KGPL_COT\n",
            "<__main__.KGPL_COT object at 0x7d6c76b49dd0>\n"
          ]
        }
      ],
      "source": [
        "def train(cfg, data):\n",
        "    (n_user, n_item, train_data, eval_data, test_data) = data\n",
        "\n",
        "    adj_entity = train_data.entity_adj\n",
        "    adj_relation = train_data.relation_adj\n",
        "    n_entity = adj_entity.shape[0]\n",
        "    n_relation = len(torch.unique(adj_relation.reshape(-1)))\n",
        "    path_list_dict = train_data.path_list_dict\n",
        "\n",
        "    print(f\"num train records: {len(train_data)}\")\n",
        "    print(f\"num adj entities: {len(adj_entity)}, num entities: {n_entity}\")\n",
        "    print(f\"num adj relations: {len(adj_relation)}, num relations: {n_relation}\")\n",
        "\n",
        "    model = KGPL_COT(\n",
        "        cfg,\n",
        "        n_user,\n",
        "        n_item,\n",
        "        n_entity,\n",
        "        n_relation,\n",
        "        adj_entity,\n",
        "        adj_relation,\n",
        "        path_list_dict,\n",
        "        train_data,\n",
        "        eval_data,\n",
        "    )\n",
        "\n",
        "    _pos_inds = train_data[:, 2] == 1\n",
        "    train_data = train_data[_pos_inds]\n",
        "    print(\"model type:\", cfg.model_type)\n",
        "\n",
        "    topk_config = topk_settings(\n",
        "        train_data,\n",
        "        eval_data,\n",
        "        test_data,\n",
        "        n_item,\n",
        "        test_mode=True,\n",
        "        user_num=cfg.evaluate['user_num_topk'],\n",
        "    )\n",
        "\n",
        "    batch_size = cfg.optimize['batch_size']\n",
        "\n",
        "    print(model)\n",
        "\n",
        "train(cfg, data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "kgpl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
